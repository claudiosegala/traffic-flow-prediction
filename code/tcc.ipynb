{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "tcc.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "tD29g6RmWyjg",
        "xw1p5NfaMKTk",
        "rlQKvD3_tqxd",
        "0CR6HLKUUWvh"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/claudiosegala/Monografia/blob/master/code/tcc.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zxA-9RZf1SXs",
        "colab_type": "text"
      },
      "source": [
        "# TCC\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "krd1GCLFBNn3",
        "colab_type": "text"
      },
      "source": [
        "## Mount Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ebzVayUNBP2L",
        "colab_type": "code",
        "outputId": "cf7e7004-b3eb-42be-95e4-e7251eb5f16e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "import google as g # To connect with google drive\n",
        "g.colab.drive.mount('/content/drive')"
      ],
      "execution_count": 480,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tD29g6RmWyjg",
        "colab_type": "text"
      },
      "source": [
        "## Retrieve and Instanciate Dependencies\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FebihZqH2eDt",
        "colab_type": "text"
      },
      "source": [
        "For this work, we will need these libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uDUNNEdAWJPx",
        "colab_type": "code",
        "outputId": "360a0c82-0348-4bfc-e799-bc0cc779e9d9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "source": [
        "!pip install tensorflow==1.15\n",
        "!pip install pandas\n",
        "!pip install numpy\n",
        "!pip install sklearn\n",
        "!pip install keras\n",
        "!pip install statsmodels"
      ],
      "execution_count": 481,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorflow==1.15 in /usr/local/lib/python3.6/dist-packages (1.15.0)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15) (1.11.2)\n",
            "Requirement already satisfied: tensorflow-estimator==1.15.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15) (1.15.1)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15) (1.1.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15) (1.0.8)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15) (3.1.0)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15) (3.10.0)\n",
            "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15) (1.17.3)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15) (0.8.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15) (0.33.6)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15) (1.1.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15) (1.12.0)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15) (1.15.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15) (0.1.7)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15) (0.8.1)\n",
            "Requirement already satisfied: gast==0.2.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15) (0.2.2)\n",
            "Requirement already satisfied: tensorboard<1.16.0,>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15) (1.15.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.8->tensorflow==1.15) (2.8.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow==1.15) (41.4.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15) (3.1.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15) (0.16.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (0.25.3)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas) (2018.9)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from pandas) (1.17.3)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from pandas) (2.6.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.6.1->pandas) (1.12.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (1.17.3)\n",
            "Requirement already satisfied: sklearn in /usr/local/lib/python3.6/dist-packages (0.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from sklearn) (0.21.3)\n",
            "Requirement already satisfied: numpy>=1.11.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->sklearn) (1.17.3)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->sklearn) (0.14.0)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->sklearn) (1.3.1)\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.6/dist-packages (2.2.5)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from keras) (1.3.1)\n",
            "Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.6/dist-packages (from keras) (1.0.8)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from keras) (1.1.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras) (2.8.0)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.6/dist-packages (from keras) (1.17.3)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras) (3.13)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from keras) (1.12.0)\n",
            "Requirement already satisfied: statsmodels in /usr/local/lib/python3.6/dist-packages (0.10.1)\n",
            "Requirement already satisfied: numpy>=1.11 in /usr/local/lib/python3.6/dist-packages (from statsmodels) (1.17.3)\n",
            "Requirement already satisfied: patsy>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from statsmodels) (0.5.1)\n",
            "Requirement already satisfied: pandas>=0.19 in /usr/local/lib/python3.6/dist-packages (from statsmodels) (0.25.3)\n",
            "Requirement already satisfied: scipy>=0.18 in /usr/local/lib/python3.6/dist-packages (from statsmodels) (1.3.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from patsy>=0.4.0->statsmodels) (1.12.0)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.19->statsmodels) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.19->statsmodels) (2.6.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wD11PvxQXFT0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sklearn\n",
        "import time\n",
        "import random\n",
        "import copy\n",
        "\n",
        "import pandas as pd # data manipulation library\n",
        "import numpy as np # math library\n",
        "\n",
        "import sklearn.metrics as sklm # metrics\n",
        "import statsmodels as sm # statistical models\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation\n",
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "from sklearn.model_selection import GridSearchCV"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xw1p5NfaMKTk",
        "colab_type": "text"
      },
      "source": [
        "## Configurations\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wwj5bYigCjxD",
        "colab_type": "text"
      },
      "source": [
        "Make the environment reproducible"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l2UhjEKrdBy8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf # machine learning library\n",
        "import os\n",
        "\n",
        "os.environ['PYTHONHASHSEED'] = '0'\n",
        "tf.reset_default_graph()\n",
        "tf.set_random_seed(0)\n",
        "np.random.seed(0)\n",
        "random.seed(0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MnTstAj_cUaL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QhJmwe0cCrF_",
        "colab_type": "text"
      },
      "source": [
        "Set path for the folder in which everything should be stored."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_sS94hovWAbv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "PATH = '/content/drive/My Drive/TCC/' # ''"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rlQKvD3_tqxd",
        "colab_type": "text"
      },
      "source": [
        "## Util\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nTkjUAUr8Vur",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class WalkingForwardTimeSeriesSplit():\n",
        "    def __init__(self, n_splits):\n",
        "        self.n_splits = n_splits\n",
        "    \n",
        "    def get_n_splits(self, X, y, groups):\n",
        "        return self.n_splits\n",
        "    \n",
        "    def split(self, X, y=None, groups=None):\n",
        "        n_samples = len(X)\n",
        "        k_fold_size = n_samples // self.n_splits\n",
        "        indices = np.arange(n_samples)\n",
        "\n",
        "        margin = 0\n",
        "        for i in range(self.n_splits):\n",
        "            start = i * k_fold_size\n",
        "            stop = start + k_fold_size\n",
        "            mid = int(0.8 * (stop - start)) + start\n",
        "            yield indices[start: mid], indices[mid + margin: stop]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0CR6HLKUUWvh",
        "colab_type": "text"
      },
      "source": [
        "## Dataset Generation Util"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KtUCy9tk_qK-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def retrieve_data(flow_interval):\n",
        "    path = f\"{PATH}dataset/dataset_flow_{flow_interval}.csv\"\n",
        "    data = pd.read_csv(path, ';')\n",
        "    \n",
        "    data['Flow'].apply(int)\n",
        "    data['AveSpeed'].apply(float)\n",
        "    data['Density'].apply(float)\n",
        "    data['Sunday'].apply(int)\n",
        "    data['Monday'].apply(int)\n",
        "    data['Tuesday'].apply(int)\n",
        "    data['Wednesday'].apply(int)\n",
        "    data['Thursday'].apply(int)\n",
        "    data['Friday'].apply(int)\n",
        "    data['Saturday'].apply(int)\n",
        "      \n",
        "    return data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8BkdL8EE5pl3",
        "colab_type": "text"
      },
      "source": [
        "## Storage Util"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fUD5PEa0SDJM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import json"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9OWaLFzKSG2A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def print_json (obj):\n",
        "  print(json.dumps(obj, sort_keys=True, indent=4))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SeBEPq-GmCvf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def store(obj, path, name):\n",
        "  with open(f\"{PATH}{path}/{name}.json\", 'w') as json_file:\n",
        "    json.dump(obj, json_file, sort_keys=True, indent=4)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y4OjfpS8R-ZC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def store_results ():\n",
        "  name = int(time.time())\n",
        "  \n",
        "  result_data['meta'] = {\n",
        "    \"SEEABLE_PAST\": SEEABLE_PAST,\n",
        "    \"PREDICT_IN_FUTURE\": PREDICT_IN_FUTURE,\n",
        "    \"FLOW_INTERVAL\": FLOW_INTERVAL,\n",
        "    \"N_SPLITS\": N_SPLITS,\n",
        "  }\n",
        "\n",
        "  store(result_data, \"results\", f\"{name}\")\n",
        "\n",
        "  slim_result_data = copy.deepcopy(result_data)\n",
        "  for model in slim_result_data['results']:\n",
        "      del slim_result_data['results'][model]['raw']\n",
        "\n",
        "  store(slim_result_data, \"results\", f\"{name}_slim.json\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K_UiuJG3GwME",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def store_comparisons (title):\n",
        "  name = str(int(time.time()))\n",
        "  \n",
        "  j = copy.deepcopy(comparison_data)\n",
        "\n",
        "  store(j, \"results/comparison\", f\"{title}_{name}\")\n",
        "    \n",
        "  for i in range(len(j)):\n",
        "    for model in j[i]['results']:\n",
        "      del j[i]['results'][model]['raw']\n",
        "\n",
        "  store(j, \"results/comparison\", f\"{title}_{name}_slim\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cybdNn4R3AW6",
        "colab_type": "text"
      },
      "source": [
        "## Models Util"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sjtwfkg0vx34",
        "colab_type": "text"
      },
      "source": [
        "### Dropped"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5atl1RAeso-d",
        "colab_type": "text"
      },
      "source": [
        "#### Random (Baseline)\n",
        "\n",
        "This implementation just guess a random number in the [0, 100] interval for every output."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H4uy_XVyswCC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def random_guess_univariate (data):\n",
        "  global result_data\n",
        "  \n",
        "  X, Y = generate_dataset(data, False, FLOW_INTERVAL, N_STEPS, N_FUTURE)\n",
        "\n",
        "  name = \"Random Guess\"\n",
        "  m = max(Y)\n",
        "\n",
        "  expected, observed, times = [], [], []\n",
        "  pointers = split_dataset(len(Y), SET_SPLIT, TEST_SPLIT)\n",
        "  \n",
        "  for i, j, k in pointers:\n",
        "    start = time.time()\n",
        "\n",
        "    Y_hat = [random.randint(0, m) for i in range(k - j)]\n",
        "\n",
        "    expected.append(Y[j:k])\n",
        "    observed.append(Y_hat)\n",
        "    times.append(time.time() - start)\n",
        "\n",
        "  result_data['results'][name] = evaluate(expected, observed, times, name)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FdvOTuZWwxcq",
        "colab_type": "text"
      },
      "source": [
        "#### ARIMA\n",
        "\n",
        "This implementation was based on [How to Create an ARIMA Model for Time Series Forecasting in Python](https://machinelearningmastery.com/arima-for-time-series-forecasting-with-python/)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aFyNYnZ8wweq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def arima (X):\n",
        "  size = int(len(X) * TRAIN_SPLIT)\n",
        "  acc = X[size-(2*WEEK_SIZE):size]\n",
        "  Y = X[size+N_FUTURE:]\n",
        "  Y_hat = []\n",
        "  \n",
        "  #for t in range(len(Y)):\n",
        "  for t in range(50):\n",
        "    print(t, len(Y))\n",
        "    \n",
        "    model = sm.tsa.arima_model.ARIMA(acc, order=(5, 1, 0))\n",
        "    model_fit = model.fit(disp=0)\n",
        "    \n",
        "    start = len(acc)\n",
        "    end = start + N_FUTURE\n",
        "    \n",
        "    prediction = model_fit.predict(start=start, end=end+1)\n",
        "    \n",
        "    print(prediction)\n",
        "    Y_hat.append(prediction[-1])    \n",
        "    acc.append(X[size + t])\n",
        "    acc.pop(0)\n",
        "  \n",
        "  print_difference(Y, Y_hat)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZGgV2YkoYA_W",
        "colab_type": "text"
      },
      "source": [
        "#### Logistic Regression\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WbUu54z8YS0b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KvfgRi1GYD7K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def logistic_regression(data, useB):\n",
        "  global result_data\n",
        "  \n",
        "  name = \"LR B\" if useB else \"LR A\"\n",
        "\n",
        "  expected, observed, times = [], [], []\n",
        "\n",
        "  X, Y = generate_dataset(data, useB, FLOW_INTERVAL, N_STEPS, N_FUTURE)\n",
        "  X = X.reshape(X.shape[0], X.shape[1] * X.shape[2])\n",
        "\n",
        "  model = LogisticRegression()\n",
        "\n",
        "  pointers = split_dataset(len(X), SET_SPLIT, TEST_SPLIT)\n",
        "  \n",
        "  for i, j, k in pointers:\n",
        "    start = time.time()\n",
        "    \n",
        "    model.fit(X[i:j], Y[i:j])\n",
        "    \n",
        "    expected.append(Y[j:k])\n",
        "    observed.append(model.predict(X[j:k]))\n",
        "    times.append(time.time() - start)\n",
        "    \n",
        "  result_data['results'][name] = evaluate(expected, observed, times, name)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B3dQ_nM_TgDC",
        "colab_type": "text"
      },
      "source": [
        "#### RNN\n",
        "\n",
        "The optimzation was based on [How to Grid Search Hyperparameters for Deep Learning Models in Python With Keras](https://machinelearningmastery.com/grid-search-hyperparameters-deep-learning-models-python-keras/)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BH_62Q8G2s9a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.layers import SimpleRNN"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "st9EurMVT5gj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def rnn (data, useB): \n",
        "  global result_data\n",
        "  \n",
        "  name = \"RNN B\" if useB else \"RNN A\"\n",
        "  \n",
        "  X, Y = generate_dataset(data, useB, FLOW_INTERVAL, N_STEPS, N_FUTURE)\n",
        "  \n",
        "  expected, observed, times = [], [], []\n",
        "  \n",
        "  model = Sequential()\t\t\n",
        "\n",
        "  model.add(SimpleRNN(50, activation='relu', input_shape=(X.shape[1], X.shape[2])))\t\t\n",
        "  model.add(Dense(1))\t\t\n",
        "\n",
        "  model.compile(optimizer='adam', loss='mse', metrics = [\"accuracy\"])\n",
        "  \n",
        "  pointers = split_dataset(len(X), SET_SPLIT, TEST_SPLIT)\n",
        "  \n",
        "  for i, j, k in pointers:\n",
        "    start = time.time()\n",
        "    \n",
        "    model.fit(X[i:j], Y[i:j], validation_split=0.2, batch_size=64, epochs=15, verbose=0)\n",
        "    \n",
        "    expected.append(Y[j:k])\n",
        "    observed.append(model.predict(X[j:k]))\n",
        "    times.append(time.time() - start)\n",
        "    \n",
        "  result_data['results'][name] = evaluate(expected, observed, times, name)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QYQ-2RhUqtZu",
        "colab_type": "text"
      },
      "source": [
        "### Misc\n",
        "\n",
        "Function to help implement the training and evaluation of the models.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "65Zypk5Cbiv0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate_precision_hit_ratio (Y, Y_hat):\n",
        "  \"\"\" Trend Prediction Ratio Calculation\n",
        "  \n",
        "  Calculates the ratio of up/down prediction.\n",
        "  \n",
        "  Arguments:\n",
        "    Y: the expected dataset.\n",
        "    Y_hat: the observed dataset.\n",
        "  \"\"\"\n",
        "  \n",
        "  cnt = 0\n",
        "  \n",
        "  for i in range(len(Y)):\n",
        "    if i < N_FUTURE:\n",
        "      continue\n",
        "      \n",
        "    exp = Y[i] - Y[i - N_FUTURE]\n",
        "    obs = Y_hat[i] - Y[i - N_FUTURE]\n",
        "    \n",
        "    if exp * obs > 0:\n",
        "      cnt += 1\n",
        "    \n",
        "  return cnt / len(Y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gTes33wAe1zg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate_precision_bucket (Y, Y_hat):\n",
        "  \"\"\" Precision Bucket Calculation\n",
        "  \n",
        "  Counts how many of the prediction got wronng by at most 2ˆx, x \n",
        "  being the bucket. There are 7 buckets, that is, the maximum error \n",
        "  calculated is 128.\n",
        "  \n",
        "  Arguments:\n",
        "    Y: the expected dataset.\n",
        "    Y_hat: the observed dataset.\n",
        "  \"\"\"\n",
        "  \n",
        "  n = 7 # the number of buckets\n",
        "  buckets = [0] * n\n",
        "  \n",
        "  for i in range(len(Y)):\n",
        "    diff = abs(Y[i] - Y_hat[i])\n",
        "    \n",
        "    for i in range (n):\n",
        "      if diff <= 2**i:\n",
        "        buckets[i] += 1\n",
        "        break\n",
        "\n",
        "  for i in range (n):\n",
        "     buckets[i] = buckets[i] / len(Y)\n",
        "\n",
        "  return tuple(buckets)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-u6hphOb8AQQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate_raw (expected, observed, times):\n",
        "  \"\"\" Evaluate Raw Sessions \n",
        "  \n",
        "  Evaluate each of the train&test sessions by RMSE, NRMSE, MAE, HR, PRE. \n",
        "  It will store the results in a object and return it.\n",
        "  \n",
        "  Arguments:\n",
        "    expected: an array of expected instances of each train&test session.\n",
        "    observed: an array of observed instances of each train&test session.\n",
        "    times: an array of the time of each train&test session.\n",
        "  \"\"\"\n",
        "  \n",
        "  n = len(expected)\n",
        "\n",
        "  for i in range(n):\n",
        "    observed[i] = [0 if isnan(o) else o for o in observed[i]]\n",
        "\n",
        "  for i in range(n):\n",
        "    observed[i] = [max(o, 0) for o in observed[i]]\n",
        "  \n",
        "  raw = {\n",
        "    'expected': expected,\n",
        "    'observed': observed,\n",
        "    'TIME': times,\n",
        "    'RMSE': [0] * n,\n",
        "    # 'NRMSE': [0] * n,\n",
        "    'MAE': [0] * n,\n",
        "    'HR': [0] * n,\n",
        "    #'PRE': [0] * n,\n",
        "  }\n",
        "  \n",
        "  for i in range(n):\n",
        "    Y = expected[i]\n",
        "    Y_hat = observed[i]\n",
        "    time = times[i]\n",
        "\n",
        "    raw['MAE'][i] = sklm.mean_absolute_error(Y, Y_hat)\n",
        "    raw['RMSE'][i] = np.sqrt(sklm.mean_squared_error(Y, Y_hat))\n",
        "    # raw['NRMSE'][i] = raw['RMSE'][i] / np.std(Y)\n",
        "    raw['HR'][i] = evaluate_precision_hit_ratio(Y, Y_hat)\n",
        "    #raw['PRE'][i] = evaluate_precision_bucket(Y, Y_hat)\n",
        "    \n",
        "    if VERBOSITY:\n",
        "      print(f\"({i+1}/{n}) Test Size: {len(Y)}, Time: {time}s\")\n",
        "      print(f\"\\tRMSE: {raw['RMSE'][i]}\")\n",
        "      # print(f\"\\tNRMSE: {raw['NRMSE'][i]}\")\n",
        "      print(f\"\\tMAE: {raw['MAE'][i]}\")\n",
        "      print(f\"\\tHit Ratio: {raw['HR'][i] * 100}%\")\n",
        "\n",
        "  return raw"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nW3-LAmDNpYL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate (expected, observed, times, name):\n",
        "  \"\"\" Evaluate Sessions\n",
        "  \n",
        "  Evaluate models by RMSE, NRMSE, MAE, HR, PRE. It will store the \n",
        "  results in a object and return it.\n",
        "  \n",
        "  Arguments:\n",
        "    expected: an array of expected instances of each \n",
        "      train&test session.\n",
        "    observed: an array of observed instances of each \n",
        "      train&test session.\n",
        "    times: an array of the time of each train&test session.\n",
        "    name: the name of the model\n",
        "  \"\"\"\n",
        "  n = len(expected)\n",
        "  flatten = lambda l : [i for sl in l for i in sl]\n",
        "  \n",
        "  # Make the arrays serializable\n",
        "  expected = list(map(list, expected))\n",
        "  observed = list(map(list, observed))\n",
        "  \n",
        "  for i in range(n):\n",
        "    expected[i] = list(map(float, expected[i]))\n",
        "    observed[i] = list(map(float, observed[i]))\n",
        "  \n",
        "  raw = evaluate_raw(expected, observed, times)\n",
        "  \n",
        "  #n_buckets = len(raw['PRE'])\n",
        "  #_pre = [[pre[i] for pre in raw['PRE']] for i in range(n_buckets)]\n",
        "  \n",
        "  eva = {\n",
        "    'TIME': int(sum(times)),\n",
        "    'RMSE': float(np.mean(raw['RMSE'])),\n",
        "    # 'NRMSE': float(np.mean(raw['NRMSE'])),\n",
        "    'MAE': float(np.mean(raw['MAE'])),\n",
        "    'HR': float(np.mean(raw['HR'])),\n",
        "    #'PRE': [float(np.mean(p)) for p in _pre],\n",
        "    'has_negative': (min(flatten(observed)) < 0),\n",
        "    'raw': raw\n",
        "  }\n",
        "  \n",
        "  print(f\"\\n{name} Final Result:\")\n",
        "  print(f\"\\tTotal Time: {eva['TIME']}s\")\n",
        "  print(f\"\\tRMSE: {eva['RMSE']}\")\n",
        "  # print(f\"\\tNRMSE: {eva['NRMSE']}\")\n",
        "  print(f\"\\tMAE: {eva['MAE']}\")\n",
        "  print(f\"\\tHit Ratio: {eva['HR'] * 100}%\")\n",
        "  #print(f\"\\tPrecision: {eva['PRE']}\")\n",
        "    \n",
        "  return eva"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4MsKbDOXEQho",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_dataset(data, useB, n_steps, n_future):\n",
        "  \"\"\" Generate Dataset\n",
        "  \n",
        "  Generate a dataset provided a sequence. Reshape the sequence in rolling intervals from [samples, timesteps] into \n",
        "  [samples, timesteps, features] and split the sequence. The split the sequence in rolling intervals with a corresponding value \n",
        "  like the example bellow.\n",
        "\n",
        "  Ex: split_sequence([1, 2, 3, 4, 5], 3) #([[1, 2, 3], [2, 3, 4]], [4, 5])\n",
        "  \n",
        "  Arguments:\n",
        "    raw_seq: the sequence to reshape.\n",
        "    useB: if the dataset is more complex or not.\n",
        "    n_steps: size of the rolling interval\n",
        "    n_future: the distance to the interval the value should be.  \n",
        "  \"\"\"\n",
        "\n",
        "  sequence = np.array(data if useB else data['Flow'])\n",
        "\n",
        "  n = len(sequence)\n",
        "  X, Y = list(), list()\n",
        "\n",
        "  for i in range(n):\n",
        "    j = i + n_steps\n",
        "    k = j + n_future\n",
        "\n",
        "    if k >= n:\n",
        "      break\n",
        "\n",
        "    seq_x, seq_y = sequence[i:j], sequence[k]\n",
        "    X.append(seq_x)\t\n",
        "    Y.append(seq_y[0] if useB else seq_y)\n",
        "\n",
        "  X, Y = np.array(X), np.array(Y)\t\n",
        "  \n",
        "  if not useB:\n",
        "    X = X.reshape((X.shape[0], X.shape[1], 1))\n",
        "\n",
        "  return X, Y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j2hH4Og160vf",
        "colab_type": "text"
      },
      "source": [
        "### Moving Average (Baseline)\n",
        "\n",
        "This implementation just get the mean of every flow value in the input and place it as output."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7tffuEQJ63Sv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def moving_average (data):\n",
        "  global result_data\n",
        "\n",
        "  name = \"Moving Average\"\n",
        "  \n",
        "  X, Y = generate_dataset(data, False, N_STEPS, N_FUTURE)\n",
        "\n",
        "  cv = WalkingForwardTimeSeriesSplit(n_splits=N_SPLITS)\n",
        "  expected, observed, times = [], [], []\n",
        "\n",
        "  for train_index, test_index in cv.split(X):\n",
        "    X_test = X[test_index]\n",
        "    Y_test = Y[test_index]\n",
        "  \n",
        "    start_time = time.time()\n",
        "    Y_hat = [np.mean(x) for x in X_test]\n",
        "    end_time = time.time()\n",
        "    \n",
        "    expected.append(Y_test)\n",
        "    observed.append(Y_hat)\n",
        "    times.append(end_time - start_time)\n",
        "    \n",
        "  result_data['results'][name] = evaluate(expected, observed, times, name)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HwDC7WBEOf_H",
        "colab_type": "text"
      },
      "source": [
        "### Naive (Baseline)\n",
        "\n",
        "This implementation just use the last value of input as output."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w1V9y_fZOsj1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def naive (data):\n",
        "  global result_data\n",
        "\n",
        "  name = \"Naive\"\n",
        "  \n",
        "  X, Y = generate_dataset(data, False, N_STEPS, N_FUTURE)\n",
        "  X = X.reshape(X.shape[0], X.shape[1])\n",
        "\n",
        "  cv = WalkingForwardTimeSeriesSplit(n_splits=N_SPLITS)\n",
        "  expected, observed, times = [], [], []\n",
        "\n",
        "  for train_index, test_index in cv.split(X):\n",
        "    X_test = X[test_index]\n",
        "    Y_test = Y[test_index]\n",
        "  \n",
        "    start_time = time.time()\n",
        "    Y_hat = [x[-1] for x in X_test]\n",
        "    end_time = time.time()\n",
        "    \n",
        "    expected.append(Y_test)\n",
        "    observed.append(Y_hat)\n",
        "    times.append(end_time - start_time)\n",
        "    \n",
        "  result_data['results'][name] = evaluate(expected, observed, times, name)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GVUHvQxwWVrW",
        "colab_type": "text"
      },
      "source": [
        "### Random Forest\n",
        "\n",
        "This implementation is based on [Random Forest Algorithm with Python and Scikit-Learn](https://stackabuse.com/random-forest-algorithm-with-python-and-scikit-learn/)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fkk54MXRd6K6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.ensemble import RandomForestRegressor"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ouDTRhcmvgWL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def random_forest_grid(data, useB):\n",
        "  global result_data\n",
        "\n",
        "  name = \"RF Grid B\" if useB else \"RF Grid A\"\n",
        "  \n",
        "  X, Y = generate_dataset(data, useB, N_STEPS, N_FUTURE)\n",
        "  X = X.reshape(X.shape[0], X.shape[1] * X.shape[2])\n",
        "    \n",
        "  param_grid = {\n",
        "    # 'bootstrap': [True, False],\n",
        "    'max_depth': [8, 16, 32, 64, None],\n",
        "    'n_estimators': [50, 100, 200, 400, 800],\n",
        "  }\n",
        "  model = sklearn.ensemble.RandomForestRegressor(max_features='auto', random_state=0)\n",
        "  scoring = 'neg_mean_squared_error'\n",
        "  tscv = WalkingForwardTimeSeriesSplit(n_splits=N_SPLITS)\n",
        "  grid_search = GridSearchCV(estimator=model, param_grid=param_grid, scoring=scoring, cv=tscv, n_jobs=4, verbose=2)\n",
        "\n",
        "  start_time = time.time()\n",
        "  grid_search.fit(X, Y)\n",
        "  end_time = time.time()\n",
        "\n",
        "  best_model = grid_search.best_estimator_\n",
        "\n",
        "  expected, observed, times = [], [], []\n",
        "\n",
        "  cv = WalkingForwardTimeSeriesSplit(n_splits=N_SPLITS)\n",
        "\n",
        "  for train_index, test_index in cv.split(X):\n",
        "    X_train, X_test = X[train_index], X[test_index]\n",
        "    Y_train, Y_test = Y[train_index], Y[test_index]\n",
        "\n",
        "    _start_time = time.time()\n",
        "    best_model.fit(X_train, Y_train)\n",
        "    _end_time = time.time()\n",
        "    \n",
        "    expected.append(Y_test)\n",
        "    observed.append(best_model.predict(X_test))\n",
        "    times.append(_end_time - _start_time)\n",
        "\n",
        "  cv_results = copy.deepcopy(grid_search.cv_results_)\n",
        "  for key in cv_results:\n",
        "    if type(cv_results[key]) != list: \n",
        "      cv_results[key] = cv_results[key].tolist()\n",
        "    \n",
        "  res = evaluate(expected, observed, times, name)\n",
        "  res['time'] = end_time - start_time\n",
        "  res['params'] = grid_search.best_params_\n",
        "  res['score'] = cv_results\n",
        "\n",
        "  store(res, \"results/grid\", f\"{name}\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aphhugpvWYkP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def random_forest(data, useB):\n",
        "  global result_data\n",
        "  \n",
        "  name = \"RF B\" if useB else \"RF A\"\n",
        "  \n",
        "  model = sklearn.ensemble.RandomForestRegressor(n_estimators=100, max_features='auto', random_state=0)\n",
        "\n",
        "  X, Y = generate_dataset(data, useB, N_STEPS, N_FUTURE)\n",
        "  X = X.reshape(X.shape[0], X.shape[1] * X.shape[2])\n",
        "\n",
        "  expected, observed, times = [], [], []\n",
        "  tscv = WalkingForwardTimeSeriesSplit(n_splits=N_SPLITS)\n",
        "\n",
        "  for train_index, test_index in tscv.split(X):\n",
        "    X_train, X_test = X[train_index], X[test_index]\n",
        "    Y_train, Y_test = Y[train_index], Y[test_index]\n",
        "\n",
        "    start_time = time.time()\n",
        "    model.fit(X_train, Y_train)\n",
        "    end_time = time.time()\n",
        "    \n",
        "    expected.append(Y_test)\n",
        "    observed.append(model.predict(X_test))\n",
        "    times.append(end_time - start_time)\n",
        "    \n",
        "  result_data['results'][name] = evaluate(expected, observed, times, name)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8WICqU265GQ2",
        "colab_type": "text"
      },
      "source": [
        "### Support Vector Machine"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6O3aVV1s5KdL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn import svm"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vwoErPSUdfvt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def support_vector_machine_grid(data, useB):\n",
        "  global result_data\n",
        "\n",
        "  name = \"SVM Grid B\" if useB else \"SVM Grid A\"\n",
        "  \n",
        "  X, Y = generate_dataset(data, useB, N_STEPS, N_FUTURE)\n",
        "  X = X.reshape(X.shape[0], X.shape[1] * X.shape[2])\n",
        "    \n",
        "  param_grid = {\n",
        "    'C': [1.0, 10.0, 100.0, 1000.0],\n",
        "    'gamma': list(np.logspace(-2, 2, 4)) + ['scale'],\n",
        "    # 'epsilon': [0.01, 0.1, 1]\n",
        "  }\n",
        "  model = svm.SVR(epsilon=1.0)\n",
        "  scoring = 'neg_mean_squared_error'\n",
        "  tscv = WalkingForwardTimeSeriesSplit(n_splits=N_SPLITS)\n",
        "  grid_search = GridSearchCV(estimator=model, param_grid=param_grid, scoring=scoring, cv=tscv, n_jobs=4, verbose=2)\n",
        "\n",
        "  start_time = time.time()\n",
        "  grid_search.fit(X, Y)\n",
        "  end_time = time.time()\n",
        "\n",
        "  best_model = grid_search.best_estimator_\n",
        "\n",
        "  expected, observed, times = [], [], []\n",
        "\n",
        "  cv = WalkingForwardTimeSeriesSplit(n_splits=N_SPLITS)\n",
        "\n",
        "  for train_index, test_index in cv.split(X):\n",
        "    X_train, X_test = X[train_index], X[test_index]\n",
        "    Y_train, Y_test = Y[train_index], Y[test_index]\n",
        "\n",
        "    _start_time = time.time()\n",
        "    best_model.fit(X_train, Y_train)\n",
        "    _end_time = time.time()\n",
        "    \n",
        "    expected.append(Y_test)\n",
        "    observed.append(best_model.predict(X_test))\n",
        "    times.append(_end_time - _start_time)\n",
        "\n",
        "  cv_results = copy.deepcopy(grid_search.cv_results_)\n",
        "  for key in cv_results:\n",
        "    if type(cv_results[key]) != list: \n",
        "      cv_results[key] = cv_results[key].tolist()\n",
        "    \n",
        "  res = evaluate(expected, observed, times, name)\n",
        "  res['time'] = end_time - start_time\n",
        "  res['best_params'] = grid_search.best_params_\n",
        "  res['params'] = param_grid\n",
        "  res['score'] = cv_results\n",
        "\n",
        "  store(res, \"results/grid\", f\"{name}\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "scKSmSyk5Awm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def support_vector_machine(data, useB):\n",
        "  global result_data\n",
        "  \n",
        "  name = \"SVM B\" if useB else \"SVM A\"\n",
        "  \n",
        "  model = svm.SVR(gamma='scale', C=1.0, epsilon=0.2)\n",
        "\n",
        "  X, Y = generate_dataset(data, useB, N_STEPS, N_FUTURE)\n",
        "  X = X.reshape(X.shape[0], X.shape[1] * X.shape[2])\n",
        "\n",
        "  expected, observed, times = [], [], []\n",
        "  tscv = WalkingForwardTimeSeriesSplit(n_splits=N_SPLITS)\n",
        "\n",
        "  for train_index, test_index in tscv.split(X):\n",
        "    X_train, X_test = X[train_index], X[test_index]\n",
        "    Y_train, Y_test = Y[train_index], Y[test_index]\n",
        "\n",
        "    start_time = time.time()\n",
        "    model.fit(X_train, Y_train)\n",
        "    end_time = time.time()\n",
        "    \n",
        "    expected.append(Y_test)\n",
        "    observed.append(model.predict(X_test))\n",
        "    times.append(end_time - start_time)\n",
        "    \n",
        "  result_data['results'][name] = evaluate(expected, observed, times, name)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D7Nq3PsMTjYc",
        "colab_type": "text"
      },
      "source": [
        "### LSTM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nTbbtAki29wT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.layers import LSTM"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4YwuQGFp7w5A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_lstm(input_shape):\n",
        "  def create(n=100, activation='sigmoid'):\n",
        "    model = Sequential()\t\t\n",
        "\n",
        "    model.add(LSTM(n, activation=activation, input_shape=input_shape))\t\t\n",
        "    model.add(Dense(1))\t\t\n",
        "\n",
        "    model.compile(optimizer='adam', loss='mse', metrics = [\"accuracy\"])\t\n",
        "\n",
        "    return model\n",
        "\n",
        "  return create"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZrdfXg73NSik",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def lstm_grid(data, useB):\t\t\n",
        "  global result_data\n",
        "\n",
        "  name = \"LSTM Grid B\" if useB else \"LSTM Grid A\"\n",
        "        \n",
        "  X, Y = generate_dataset(data, useB, N_STEPS, N_FUTURE)\n",
        "\n",
        "  param_grid = {\t\t\n",
        "    'n': [50, 100, 200, 400, 800],\n",
        "    'batch_size': [8, 16, 32, 64]\n",
        "  }\n",
        "  model = KerasClassifier(build_fn=create_lstm((X.shape[1], X.shape[2])), validation_split=0.2, epochs=15, verbose=0)\n",
        "  scoring = 'neg_mean_squared_error'\n",
        "  tscv = WalkingForwardTimeSeriesSplit(n_splits=N_SPLITS)\n",
        "  grid_search = GridSearchCV(estimator=model, param_grid=param_grid, scoring=scoring, cv=tscv, n_jobs=4, verbose=2)\t\t\n",
        "      \n",
        "  start_time = time.time()\n",
        "  grid_search.fit(X, Y)\n",
        "  end_time = time.time()\n",
        "\n",
        "  best_model = grid_search.best_estimator_\n",
        "\n",
        "  expected, observed, times = [], [], []\n",
        "\n",
        "  cv = WalkingForwardTimeSeriesSplit(n_splits=N_SPLITS)\n",
        "\n",
        "  for train_index, test_index in cv.split(X):\n",
        "    X_train, X_test = X[train_index], X[test_index]\n",
        "    Y_train, Y_test = Y[train_index], Y[test_index]\n",
        "\n",
        "    _start_time = time.time()\n",
        "    best_model.fit(X_train, Y_train)\n",
        "    _end_time = time.time()\n",
        "    \n",
        "    expected.append(Y_test)\n",
        "    observed.append(best_model.predict(X_test))\n",
        "    times.append(_end_time - _start_time)\n",
        "\n",
        "  cv_results = copy.deepcopy(grid_search.cv_results_)\n",
        "  for key in cv_results:\n",
        "    if type(cv_results[key]) != list: \n",
        "      cv_results[key] = cv_results[key].tolist()\n",
        "    \n",
        "  res = evaluate(expected, observed, times, name)\n",
        "  res['time'] = end_time - start_time\n",
        "  res['params'] = grid_search.best_params_\n",
        "  res['score'] = cv_results\n",
        "\n",
        "  store(res, \"results/grid\", f\"{name}\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AGyRz8GzTp4e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def lstm (data, useB): \n",
        "  global result_data\n",
        "  \n",
        "  name = \"LSTM B\" if useB else \"LSTM A\"\n",
        "\n",
        "  X, Y = generate_dataset(data, useB, N_STEPS, N_FUTURE)\n",
        "\n",
        "  model = create_lstm((X.shape[1], X.shape[2]))()\n",
        "  \n",
        "  expected, observed, times, hist = [], [], [], []\n",
        "  tscv = WalkingForwardTimeSeriesSplit(n_splits=N_SPLITS)\n",
        "\n",
        "  for train_index, test_index in tscv.split(X):\n",
        "    X_train, X_test = X[train_index], X[test_index]\n",
        "    Y_train, Y_test = Y[train_index], Y[test_index]\n",
        "  \n",
        "    start_time = time.time()\n",
        "    h = model.fit(X_train, Y_train, validation_split=0.2, batch_size=64, epochs=15, verbose=0)\n",
        "    end_time = time.time()\n",
        "\n",
        "    expected.append(Y_test)\n",
        "    observed.append(model.predict(X_test))\n",
        "    times.append(end_time - start_time)\n",
        "    hist.append(h)\n",
        "\n",
        "  result_data['results'][name] = evaluate(expected, observed, times, name)\n",
        "  result_data['results'][name]['history'] = {\n",
        "      'loss': [h.history['loss'] for h in hist],\n",
        "      'val_loss': [h.history['val_loss'] for h in hist],\n",
        "  }"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qhaufggBRABt",
        "colab_type": "text"
      },
      "source": [
        "### GRU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rP_hAZNu3CS7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.layers import GRU"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z1AKkTBu79Om",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_gru(input_shape):\n",
        "  def create(n=100, activation='sigmoid'):\n",
        "    model = Sequential()\t\t\n",
        "\n",
        "    model.add(GRU(n, activation=activation, input_shape=input_shape))\t\t\n",
        "    model.add(Dense(1))\t\t\n",
        "\n",
        "    model.compile(optimizer='adam', loss='mse', metrics = [\"accuracy\"])\t\t\n",
        "\n",
        "    return model\t\t\n",
        "    \n",
        "  return create"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ETV7D_7NtpG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def gru_grid(data, useB):\t\t\n",
        "  global result_data\n",
        "\n",
        "  name = \"GRU Grid B\" if useB else \"GRU Grid A\"\n",
        "        \n",
        "  X, Y = generate_dataset(data, useB, N_STEPS, N_FUTURE)\n",
        "\n",
        "  param_grid = {\t\t\n",
        "    'n': [50, 100, 200, 400, 800],\n",
        "    'batch_size': [8, 16, 32, 64]\n",
        "  }\n",
        "  model = KerasClassifier(build_fn=create_gru((X.shape[1], X.shape[2])), validation_split=0.2, epochs=15, verbose=0)\n",
        "  scoring = 'neg_mean_squared_error'\n",
        "  tscv = WalkingForwardTimeSeriesSplit(n_splits=N_SPLITS)\n",
        "  grid_search = GridSearchCV(estimator=model, param_grid=param_grid, scoring=scoring, cv=tscv, n_jobs=4, verbose=2)\t\t\n",
        "\n",
        "  start_time = time.time()\n",
        "  grid_search.fit(X, Y)\n",
        "  end_time = time.time()\n",
        "\n",
        "  best_model = grid_search.best_estimator_\n",
        "\n",
        "  expected, observed, times = [], [], []\n",
        "\n",
        "  cv = WalkingForwardTimeSeriesSplit(n_splits=N_SPLITS)\n",
        "\n",
        "  for train_index, test_index in cv.split(X):\n",
        "    X_train, X_test = X[train_index], X[test_index]\n",
        "    Y_train, Y_test = Y[train_index], Y[test_index]\n",
        "\n",
        "    _start_time = time.time()\n",
        "    best_model.fit(X_train, Y_train)\n",
        "    _end_time = time.time()\n",
        "    \n",
        "    expected.append(Y_test)\n",
        "    observed.append(best_model.predict(X_test))\n",
        "    times.append(_end_time - _start_time)\n",
        "\n",
        "  cv_results = copy.deepcopy(grid_search.cv_results_)\n",
        "  for key in cv_results:\n",
        "    if type(cv_results[key]) != list: \n",
        "      cv_results[key] = cv_results[key].tolist()\n",
        "    \n",
        "  res = evaluate(expected, observed, times, name)\n",
        "  res['time'] = end_time - start_time\n",
        "  res['params'] = grid_search.best_params_\n",
        "  res['score'] = cv_results\n",
        "\n",
        "  store(res, \"results/grid\", f\"{name}\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KD1GwF_b9P2h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def gru (data, useB): \n",
        "  global result_data\n",
        "  \n",
        "  name = \"GRU B\" if useB else \"GRU A\"\n",
        "\n",
        "  X, Y = generate_dataset(data, useB, N_STEPS, N_FUTURE)\n",
        "\n",
        "  model = create_gru((X.shape[1], X.shape[2]))()\n",
        "  \n",
        "  expected, observed, times, hist = [], [], [], []\n",
        "  tscv = WalkingForwardTimeSeriesSplit(n_splits=N_SPLITS)\n",
        "\n",
        "  for train_index, test_index in tscv.split(X):\n",
        "    X_train, X_test = X[train_index], X[test_index]\n",
        "    Y_train, Y_test = Y[train_index], Y[test_index]\n",
        "  \n",
        "    start_time = time.time()\n",
        "    h = model.fit(X_train, Y_train, validation_split=0.2, batch_size=64, epochs=15, verbose=0)\n",
        "    end_time = time.time()\n",
        "\n",
        "    expected.append(Y_test)\n",
        "    observed.append(model.predict(X_test))\n",
        "    times.append(end_time - start_time)\n",
        "    hist.append(h)\n",
        "  \n",
        "  result_data['results'][name] = evaluate(expected, observed, times, name)\n",
        "  result_data['results'][name]['history'] = {\n",
        "      'loss': [h.history['loss'] for h in hist],\n",
        "      'val_loss': [h.history['val_loss'] for h in hist],\n",
        "  }"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b1QmFwC1k2NO",
        "colab_type": "text"
      },
      "source": [
        "## Comparison Util"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ouhGkCmmVkH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def run_models():\n",
        "  global result_data\n",
        "  \n",
        "  result_data = {\n",
        "      'results': {},\n",
        "      'meta': {\n",
        "        'SEEABLE_PAST': SEEABLE_PAST,\n",
        "        'PREDICT_IN_FUTURE': PREDICT_IN_FUTURE,\n",
        "        'FLOW_INTERVAL': FLOW_INTERVAL,\n",
        "        'N_SPLITS': N_SPLITS,\n",
        "      }\n",
        "  }\n",
        "\n",
        "  data = retrieve_data(FLOW_INTERVAL)\n",
        "\n",
        "  moving_average(data)\n",
        "  naive(data)\n",
        "  random_forest(data, False)\n",
        "  random_forest(data, True)\n",
        "  support_vector_machine(data, False)\n",
        "  support_vector_machine(data, True)\n",
        "  lstm(data, False)\n",
        "  lstm(data, True)\n",
        "  gru(data, False)\n",
        "  gru(data, True)\n",
        "\n",
        "  store_results()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uVG-q8fvk32O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def compare_results_by_n_split(values):\n",
        "  global N_SPLITS\n",
        "  global comparison_data\n",
        "  \n",
        "  aux = N_SPLITS\n",
        "  comparison_data = []\n",
        "  \n",
        "  for value in values:\n",
        "    N_SPLITS = value\n",
        "\n",
        "    start_time = time.time()\n",
        "    run_models()\n",
        "    end_time = time.time()\n",
        "    \n",
        "    comparison_data.append(copy.deepcopy(result_data))\n",
        "\n",
        "    print(f\"({len(comparison_data)} of {len(values)}) Finished Running with N_SPLITS {value} in {end_time - start_time} seconds\")\n",
        "\n",
        "  store_comparisons('n_split_comparison')\n",
        "  \n",
        "  N_SPLITS = aux"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W6Y3gTSGI4uy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def compare_results_by_seeable_past(values):\n",
        "  global SEEABLE_PAST\n",
        "  global N_STEPS\n",
        "  global comparison_data\n",
        "  \n",
        "  aux = SEEABLE_PAST\n",
        "  comparison_data = []\n",
        "  \n",
        "  for value in values:\n",
        "    SEEABLE_PAST = value\n",
        "    N_STEPS = SEEABLE_PAST * 60 // FLOW_INTERVAL\n",
        "\n",
        "    start_time = time.time()\n",
        "    run_models()\n",
        "    end_time = time.time()\n",
        "    \n",
        "    comparison_data.append(copy.deepcopy(result_data))\n",
        "\n",
        "    print(f\"({len(comparison_data)} of {len(values)}) Finished Running with SEEABLE_PAST {value} in {end_time - start_time} seconds\")\n",
        "\n",
        "  store_comparisons('seeable_past_comparison')\n",
        "\n",
        "  SEEABLE_PAST = aux\n",
        "  N_STEPS = SEEABLE_PAST * 60 // FLOW_INTERVAL"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2h35ErSHQH85",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def compare_results_by_flow_interval(values):\n",
        "  global FLOW_INTERVAL\n",
        "  global N_STEPS\n",
        "  global N_FUTURE\n",
        "  global DAY_SIZE\n",
        "  global WEEK_SIZE\n",
        "  global comparison_data\n",
        "  \n",
        "  aux = FLOW_INTERVAL\n",
        "  comparison_data = []\n",
        "  \n",
        "  for value in values:\n",
        "    FLOW_INTERVAL = value\n",
        "    N_STEPS = SEEABLE_PAST * 60 // FLOW_INTERVAL\n",
        "    N_FUTURE = PREDICT_IN_FUTURE * 60 // FLOW_INTERVAL\n",
        "    DAY_SIZE = (24 * 3600) // FLOW_INTERVAL  \n",
        "    WEEK_SIZE = 7 * DAY_SIZE\n",
        "\n",
        "    start_time = time.time()\n",
        "    run_models()\n",
        "    end_time = time.time()\n",
        "    \n",
        "    comparison_data.append(copy.deepcopy(result_data))\n",
        "\n",
        "    print(f\"({len(comparison_data)} of {len(values)}) Finished Running with FLOW_INTERVAL {value} in {end_time - start_time} seconds\")\n",
        "\n",
        "  store_comparisons('flow_interval_comparison')\n",
        "  \n",
        "  FLOW_INTERVAL = aux\n",
        "  N_STEPS = SEEABLE_PAST * 60 // FLOW_INTERVAL\n",
        "  N_FUTURE = PREDICT_IN_FUTURE * 60 // FLOW_INTERVAL\n",
        "  DAY_SIZE = (24 * 3600) // FLOW_INTERVAL  \n",
        "  WEEK_SIZE = 7 * DAY_SIZE"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5wC3yGkZQGFU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def compare_results_by_predict_in_future(values):\n",
        "  global PREDICT_IN_FUTURE\n",
        "  global N_FUTURE\n",
        "  global comparison_data\n",
        "  \n",
        "  aux = PREDICT_IN_FUTURE\n",
        "  comparison_data = []\n",
        "  \n",
        "  for value in values:\n",
        "    PREDICT_IN_FUTURE = value\n",
        "    N_FUTURE = PREDICT_IN_FUTURE * 60 // FLOW_INTERVAL\n",
        "\n",
        "    start_time = time.time()\n",
        "    run_models()\n",
        "    end_time = time.time()\n",
        "    \n",
        "    comparison_data.append(copy.deepcopy(result_data))\n",
        "\n",
        "    print(f\"({len(comparison_data)} of {len(values)}) Finished Running with PREDICT_IN_FUTURE {value} in {end_time - start_time} seconds\")\n",
        "\n",
        "  store_comparisons('predict_future_comparison')\n",
        "  \n",
        "  PREDICT_IN_FUTURE = aux\n",
        "  N_FUTURE = PREDICT_IN_FUTURE * 60 // FLOW_INTERVAL"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1PypQpgeb3Nu",
        "colab_type": "text"
      },
      "source": [
        "## Train&Test\n",
        "\n",
        "Run all the models and store the results at the end"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IURi5r5HV5oZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Model Parameters\n",
        "SEEABLE_PAST = 180 # in minutes\n",
        "PREDICT_IN_FUTURE = 15 # in minutes\n",
        "FLOW_INTERVAL = 150 # the interval size for each flow\n",
        "N_SPLITS = 4"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rE_V1sw0V2DL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Derivated Model Parameters\n",
        "N_STEPS = SEEABLE_PAST * 60 // FLOW_INTERVAL # the number of flows to see in the past\n",
        "N_FUTURE = PREDICT_IN_FUTURE * 60 // FLOW_INTERVAL # how much in the future we want to predict (0 = predict the flow on the next FLOW_INTERVAL minutes)\n",
        "DAY_SIZE = (24 * 60 * 60) // FLOW_INTERVAL  \n",
        "WEEK_SIZE = (7 * 24 * 60 * 60) // FLOW_INTERVAL\n",
        "VERBOSITY = True"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OnwtPTLWVHLx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "result_data = {\n",
        "    'results': {},\n",
        "    'meta': {}\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4J0XhMvkAArB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data = retrieve_data(FLOW_INTERVAL)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vIefKT5hAyXi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# moving_average(data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mNBWt3dCA0Vk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# naive(data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mXUILTmKC4uk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# random_forest(data, False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EXmwQlAOO_zQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# random_forest_grid(data, False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8DNp5XV7fkFC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# random_forest_grid(data, True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5A_g_dz9ImKb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# support_vector_machine(data, False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tcwtR-TuODBw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# support_vector_machine_grid(data, False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tl1iUSs1OEge",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# support_vector_machine_grid(data, True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aTMT-AoOIsW4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lstm(data, False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_CJh4eqDOInH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# lstm_grid(data, False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "djbiudehOJ5O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# lstm_grid(data, True)\t"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qr-K5Y9hI2k9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "gru(data, False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oR7RZ93cOK-U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# gru_grid(data, False)\t"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NQVE4Ov3OMFW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# gru_grid(data, True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8DrFskPWmsp8",
        "colab_type": "text"
      },
      "source": [
        "## Compare"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iqldR6XomuvM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "VERBOSITY = False"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n384b75iQ6Tw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predict_futures = [15, 30, 45, 60]\n",
        "compare_results_by_predict_in_future(predict_futures)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vGrQ0gM0KhLG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "flow_intervals = [150, 300, 450]\n",
        "compare_results_by_flow_interval(flow_intervals)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TOdpUrt2Qbwk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "seeable_pasts = [60, 120, 240, 480]\n",
        "compare_results_by_seeable_past(seeable_pasts)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7cRR0WElOJdS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "n_splits = [1, 2, 4, 8]\n",
        "compare_results_by_n_split(n_splits)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rBjqtHAo_aFw",
        "colab_type": "text"
      },
      "source": [
        "## Observations:\n",
        "\n",
        "+ For the evaluation of the RNN and it's variations was used the Walking Forward methodology so that we had many test sessions and all training sessions where the same size [[1]](https://towardsdatascience.com/time-series-nested-cross-validation-76adba623eb9)\n",
        "+ To remove the cross-validation of the GridSearchCV we based on the answer in [scikit learn discussion - allow GridSearchCV to work with params={} or cv=1](https://github.com/scikit-learn/scikit-learn/issues/2048)\n",
        "+ Grid Search on Keras was based on the article [How to Grid Search Hyperparameters for Deep Learning Models in Python With Keras](https://machinelearningmastery.com/grid-search-hyperparameters-deep-learning-models-python-keras/)"
      ]
    }
  ]
}