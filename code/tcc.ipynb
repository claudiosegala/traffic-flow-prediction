{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "tcc.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/claudiosegala/Monografia/blob/master/code/tcc.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tD29g6RmWyjg",
        "colab_type": "text"
      },
      "source": [
        "# Install Dependencies\n",
        "\n",
        "In this phase we have to download all the dependencies that our code will need"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uDUNNEdAWJPx",
        "colab_type": "code",
        "outputId": "313f319e-d2ee-4da9-c99c-b4500fd11ec7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 816
        }
      },
      "source": [
        "!pip install tensorflow\n",
        "!pip install pandas\n",
        "!pip install matplotlib\n",
        "!pip install numpy\n",
        "!pip install sklearn\n",
        "!pip install keras"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.6/dist-packages (1.14.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.1.7)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.1.0)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.11.2)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.7.1)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.8.0)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (3.7.1)\n",
            "Requirement already satisfied: tensorflow-estimator<1.15.0rc0,>=1.14.0rc0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.14.0)\n",
            "Requirement already satisfied: numpy<2.0,>=1.14.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.16.4)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.33.4)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.12.0)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.15.0)\n",
            "Requirement already satisfied: tensorboard<1.15.0,>=1.14.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.14.0)\n",
            "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (0.2.2)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.1.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow) (1.0.8)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow) (41.2.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow) (0.15.5)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow) (3.1.1)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.6->tensorflow) (2.8.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (0.24.2)\n",
            "Requirement already satisfied: python-dateutil>=2.5.0 in /usr/local/lib/python3.6/dist-packages (from pandas) (2.5.3)\n",
            "Requirement already satisfied: pytz>=2011k in /usr/local/lib/python3.6/dist-packages (from pandas) (2018.9)\n",
            "Requirement already satisfied: numpy>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from pandas) (1.16.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.5.0->pandas) (1.12.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (3.0.3)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib) (1.1.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib) (2.5.3)\n",
            "Requirement already satisfied: numpy>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from matplotlib) (1.16.4)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib) (0.10.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib) (2.4.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from kiwisolver>=1.0.1->matplotlib) (41.2.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.1->matplotlib) (1.12.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (1.16.4)\n",
            "Requirement already satisfied: sklearn in /usr/local/lib/python3.6/dist-packages (0.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from sklearn) (0.21.3)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->sklearn) (1.3.1)\n",
            "Requirement already satisfied: numpy>=1.11.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->sklearn) (1.16.4)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->sklearn) (0.13.2)\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.6/dist-packages (2.2.4)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from keras) (1.3.1)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras) (3.13)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from keras) (1.0.8)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from keras) (1.12.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from keras) (1.1.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras) (2.8.0)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.6/dist-packages (from keras) (1.16.4)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NCSPqG1YXAfm",
        "colab_type": "text"
      },
      "source": [
        "# Define headers\n",
        "\n",
        "In this phase we have to declare all the libraries that we will use."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wD11PvxQXFT0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf # machine learning library\n",
        "import pandas as pd # data manipulation library\n",
        "import matplotlib.pyplot as plt # plot library\n",
        "import numpy as np # math library\n",
        "import datetime as dt # to discover week day\n",
        "import time as tm # to convert to seconds\n",
        "import sklearn as skl # regression templates library\n",
        "\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, precision_score, accuracy_score, max_error\n",
        "from keras.models import Sequential\n",
        "from keras.layers import LSTM, GRU, SimpleRNN, Dense"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "krd1GCLFBNn3",
        "colab_type": "text"
      },
      "source": [
        "# Mount Drive\n",
        "\n",
        "Connect to Google Drive of 'alfredcoinworth'"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ebzVayUNBP2L",
        "colab_type": "code",
        "outputId": "b4788b0d-d32c-4eb7-ba57-c1c7cc754b23",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "import google as g # To connect with google drive\n",
        "g.colab.drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fXgVfuOmDcDH",
        "colab_type": "text"
      },
      "source": [
        "# Configure Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3E2XPoL5DfY7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "FLOW_INTERVAL = 300 # the interval size for each flow\n",
        "\n",
        "N_STEPS = 20 # the number of flows to see in the past\n",
        "\n",
        "N_FUTURE = 12 # how much in the future we want to predict (0 = predict the flow on the next 5 minutes)\n",
        "\n",
        "N_FEATURES = 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "llW_Qq7sZk1U",
        "colab_type": "text"
      },
      "source": [
        "# Data Retrieval & Transformation\n",
        "\n",
        "In this phase we have to get the data stored in Google Drive and remove the columns that we won't need. Also, convert some of them to other types.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hd2t6KbqZlgU",
        "colab_type": "code",
        "outputId": "eaa988ba-6a53-4296-b183-09e8137be5cd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        }
      },
      "source": [
        "def prepare_data (data):\n",
        "  \"\"\" Prepare the data\n",
        "  \n",
        "  This will fix types of the dataframe to use time as seconds instead of string,\n",
        "  use week day instead of date as string, use speed as float instead of string. \n",
        "  Also, will drop columns that are not necessary.\n",
        "  \"\"\"\n",
        "  \n",
        "  data = data.drop(columns=['Unnamed: 0', 'Sensor', 'Max Speed', 'Size', 'Lane'])\n",
        "  \n",
        "  data['Time'] = data['Time'].apply(lambda x : tm.strptime(x, '%H:%M:%S'))\n",
        "  data['Time'] = data['Time'].apply(lambda x : dt.timedelta(hours=x.tm_hour,minutes=x.tm_min,seconds=x.tm_sec).total_seconds())\n",
        "  data['Time'] = data['Time'].apply(lambda x : int(x))\n",
        "\n",
        "  data['Date'] = pd.to_datetime(data['Date'], format='%Y/%m/%d')\n",
        "  \n",
        "  data['WeekDay'] = data['Date'].apply(lambda x : x.weekday())\n",
        "\n",
        "  data['Speed'].apply(lambda x : float(x))\n",
        "  \n",
        "  return data\n",
        "\n",
        "\n",
        "\n",
        "# Get data from Google Drive\n",
        "data = pd.read_csv('/content/drive/My Drive/TCC/chunks/chunk_00.csv', sep=',')\n",
        "data = prepare_data(data)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-c1324cc35eca>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;31m# Get data from Google Drive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/My Drive/TCC/chunks/chunk_00.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m','\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprepare_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    700\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[1;32m    701\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 702\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    703\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    704\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    427\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    428\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 429\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    430\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    431\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1120\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'c'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1121\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'c'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1122\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1123\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1124\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'python'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1851\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'usecols'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1852\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1853\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1854\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1855\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] File b'/content/drive/My Drive/TCC/chunks/chunk_00.csv' does not exist: b'/content/drive/My Drive/TCC/chunks/chunk_00.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R14KefuiBycd",
        "colab_type": "text"
      },
      "source": [
        "# Get Flow\n",
        "\n",
        "This will transform the time series of register cars that passed in a array of flow per 5 minute."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RmF8FxOBB2T0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_flow (data, interval):\n",
        "  \"\"\" Extract flow from data\n",
        "  \n",
        "  This will transform the time series of register cars that passed in a array of\n",
        "  flow per 'timeInterval' seconds.\n",
        "  \"\"\"\n",
        "  \n",
        "  date = np.asarray(data['Date'])\n",
        "  time = np.asarray(data['Time'])\n",
        "  speed = np.asarray(data['Speed'])\n",
        "  \n",
        "  sz = len(speed)\n",
        "  dateControl = date[0] #seta o controle de data com o primeiro dia do chunk\n",
        "  timeBlock = interval\n",
        "  countFlow = 0\n",
        "  flow_data = []\n",
        "\n",
        "  for i in range(sz):\n",
        "    if time[i] >= timeBlock: # init a new time block\n",
        "      flow_data.append((date[i], timeBlock, countFlow)) \n",
        "      timeBlock += interval\n",
        "      countFlow = 0 # TODO: verify if this is correct\n",
        "      \n",
        "    if date[i] > dateControl: # reset on day change\n",
        "      dateControl = date[i]\n",
        "      timeBlock = interval \n",
        "      countFlow = 0\n",
        "      \n",
        "    if time[i] < timeBlock: # add car on flow\n",
        "      countFlow += 1\n",
        "\n",
        "  #df_flow = pd.DataFrame(flow_data, columns = [\"Day\", \"TimeBlock\", \"Flow\"])\n",
        "  \n",
        "  return [f for d, t, f in flow_data ]\n",
        "\n",
        "\n",
        "raw_seq = get_flow(data, FLOW_INTERVAL)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rlQKvD3_tqxd",
        "colab_type": "text"
      },
      "source": [
        "# Plot Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5zIKDsGPtvOO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.figure(figsize=(80, 10))\n",
        "plt.plot(raw_seq)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nFv86fqFDDor",
        "colab_type": "text"
      },
      "source": [
        "# Prepare for dataset for training\n",
        "\n",
        "+ Adjust the dataset\n",
        "+ Split the dataset\n",
        "+ Create storage for the results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lr0HX0aMDKOf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def split_sequence(sequence, n_steps, n_future):\n",
        "  \"\"\" Split a univariate sequence into samples\n",
        "  \n",
        "  This function will split a sequence into many samples in the form of two\n",
        "  arrays. The first array will have as elements arrays of size n_step and the \n",
        "  second array will have as elements a integer. \n",
        "  Example:\n",
        "  \n",
        "  split_sequence([1, 2, 3, 4, 5], 3) #=> ([[1, 2, 3], [2, 3, 4]], [4, 5])\n",
        "  \"\"\"\n",
        "  \n",
        "  n = len(sequence)\n",
        "  X, Y = list(), list()\n",
        "  \n",
        "  for i in range(n):\n",
        "    # find the end of this pattern\n",
        "    end_ix = i + n_steps\n",
        "\n",
        "    # check if we are beyond the sequence\n",
        "    if end_ix + n_future > n-1:\n",
        "      break\n",
        "\n",
        "    # gather input and output parts of the pattern\n",
        "    seq_x, seq_y = sequence[i:end_ix], sequence[end_ix + n_future]\n",
        "    X.append(seq_x)\n",
        "    Y.append(seq_y)\n",
        "\n",
        "  return np.array(X), np.array(Y)\n",
        "\n",
        "\n",
        "def reshape_flow (raw_seq, n_steps, n_future, n_features):  \n",
        "  # define what is test and what is training\n",
        "  training_side = int(len(raw_seq) * 0.8)\n",
        "  \n",
        "  # split into samples\n",
        "  X_train, Y_train = split_sequence(raw_seq[:training_side], n_steps, n_future)\n",
        "  X_test, Y_test = split_sequence(raw_seq[training_side:], n_steps, n_future)\n",
        "  \n",
        "  # reshape from [samples, timesteps] into [samples, timesteps, features]\n",
        "  X_train = X_train.reshape((X_train.shape[0], X_train.shape[1], n_features))\n",
        "  X_test = X_test.reshape((X_test.shape[0], X_test.shape[1], n_features))\n",
        "  \n",
        "  return X_train, Y_train, X_test, Y_test\n",
        "\n",
        "res = {}\n",
        "X_train, Y_train, X_test, Y_test = reshape_flow(raw_seq, N_STEPS, N_FUTURE, N_FEATURES)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ED0AYCN9MAU",
        "colab_type": "text"
      },
      "source": [
        "# Train GRU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KD1GwF_b9P2h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def gru (X_train, Y_train, X_test, Y_test, n_steps, n_features): \n",
        "  # define model\n",
        "  model = Sequential()\n",
        "  model.add(GRU(50, activation='relu', input_shape=(n_steps, n_features)))\n",
        "  model.add(Dense(1))\n",
        "  \n",
        "  # compile model\n",
        "  model.compile(optimizer='adam', loss='mse', metrics = [\"accuracy\"])\n",
        "  \n",
        "  # fit model\n",
        "  model.fit(X_train, Y_train, batch_size=64, epochs=50, verbose=0) # verbose = 2\n",
        "  \n",
        "  return model.predict(X_test, verbose=0) # verbose = 2\n",
        "\n",
        "res[\"gru\"] = gru(X_train, Y_train, X_test, Y_test, N_STEPS, N_FEATURES)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WXPc_XZIJkU4",
        "colab_type": "text"
      },
      "source": [
        "# Train LSTM\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tGKJQouPJlss",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def lstm (X_train, Y_train, X_test, Y_test, n_steps, n_features): \n",
        "  # define model\n",
        "  model = Sequential()\n",
        "  model.add(LSTM(50, activation='relu', input_shape=(n_steps, n_features)))\n",
        "  model.add(Dense(1))\n",
        "  \n",
        "  # compile model\n",
        "  model.compile(optimizer='adam', loss='mse', metrics = [\"accuracy\"])\n",
        "  \n",
        "  # fit model\n",
        "  model.fit(X_train, Y_train, batch_size=64, epochs=50, verbose=0) # verbose = 2\n",
        "  \n",
        "  return model.predict(X_test, verbose=0) # verbose = 2\n",
        "\n",
        "res[\"lstm\"] = lstm(X_train, Y_train, X_test, Y_test, N_STEPS, N_FEATURES)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T94KtinQ9OF8",
        "colab_type": "text"
      },
      "source": [
        "# Train RNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CjQ4nKMG9aN9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def rnn (X_train, Y_train, X_test, Y_test, n_steps, n_features): \n",
        "  # define model\n",
        "  model = Sequential()\n",
        "  model.add(SimpleRNN(50, activation='relu', input_shape=(n_steps, n_features)))\n",
        "  model.add(Dense(1))\n",
        "  \n",
        "  # compile model\n",
        "  model.compile(optimizer='adam', loss='mse', metrics = [\"accuracy\"])\n",
        "  \n",
        "  # fit model\n",
        "  model.fit(X_train, Y_train, batch_size=64, epochs=50, verbose=0) # verbose = 2\n",
        "  \n",
        "  return model.predict(X_test, verbose=0) # verbose = 2\n",
        "\n",
        "res[\"rnn\"] = rnn(X_train, Y_train, X_test, Y_test, N_STEPS, N_FEATURES)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mX55C9T7EAdt",
        "colab_type": "text"
      },
      "source": [
        "# Test & Validate"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L9PU9_Y1ECb4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def print_metrics (Y_hat, Y_test):\n",
        "  mae = mean_absolute_error(Y_test, Y_hat)\n",
        "  mse = mean_squared_error(Y_test, Y_hat)\n",
        "  me = max_error(Y_test, Y_hat)\n",
        "  \n",
        "  print(f\"MAE: {mae}\")\n",
        "  print(f\"MSE: {mse}\")\n",
        "  print(f\"RMSE: {np.sqrt(mse)}\")\n",
        "  print(f\"Max Error: {me}\")\n",
        "  \n",
        "  \n",
        "print(\"--- GRU ---\")\n",
        "print_metrics(res[\"gru\"].round().flatten().tolist(), Y_test.tolist())\n",
        "\n",
        "print(\"--- LSTM ---\")\n",
        "print_metrics(res[\"lstm\"].round().flatten().tolist(), Y_test.tolist())\n",
        "\n",
        "print(\"--- RNN ---\")\n",
        "print_metrics(res[\"rnn\"].round().flatten().tolist(), Y_test.tolist())"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}