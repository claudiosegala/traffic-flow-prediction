# -*- coding: utf-8 -*-
"""tcc_plot.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1nT5L-1n1maMr1fEBgUSeyFb3WB-SMbRI
"""

import google as g # To connect with google drive
g.colab.drive.mount('/content/drive')

!pip install matplotlib
!pip install pandas
!pip install numpy

import json
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np

PATH = '/content/drive/My Drive/TCC/'

def plot_history (history, name):
  """ Plot of History
  
  Plot the history of loss in the training session of a model
  
  Arguments:
    history: the history returned by Keras fit of a model
    name: the name of the model
  """
  
  path = f"{PATH}plots/history/{name}"
  
  plt.plot(history.history['loss'])
  plt.plot(history.history['val_loss'])
  plt.title(name + ' Model Loss')
  plt.ylabel('Loss')
  plt.xlabel('Epoch')
  plt.legend(['train', 'test'], loc='upper left')
  plt.rcdefaults()
  
  plt.savefig(path + ".png", bbox_inches='tight')
  # plt.savefig(path + ".pdf")
  
  plt.close('all')

def plot_prediction (Y, Y_hat, title):
  """ Plot Prediction
  
  Plot the prediction (Flow x Time) of what was expected and what
  was predicted.
  """

  for i in range(len(Y)):
    name = f"{title} ({str(i+1).zfill(2)} of {len(Y)})"
    path = f"{PATH}plots/prediction/{name}"
    
    plt.plot(Y[i])
    plt.plot(Y_hat[i])
    plt.title(title + 'Prediction')
    plt.ylabel('Flow')
    plt.xlabel('Time')
    plt.legend(['actual', 'prediction'], loc='upper left')
    plt.rcdefaults()

    plt.savefig(path + ".png", bbox_inches='tight')
    # plt.savefig(path + ".pdf")

    plt.close('all')

def plot_precision_bucket (results):
  """ Plot Precision Bucket 
  
  Plot a stack box graph of the precision mesuared by the buckets.
  """
  
  path = f"{PATH}plots/precision"
  N = len(results)
  ind = np.arange(N)    # the x locations for the groups
  width = 0.35       # the width of the bars: can also be len(x) sequence
  pre, bott = [], []
  models = list(results.keys())
  n_buckets = len(results[models[0]]['PRE'])
    
  for i in range(n_buckets):
    pre.append([v["PRE"][i] for v in results.values()])
    
    if i == 0:
      bott.append([0] * N)
    else:
      bott.append([bott[i-1][j] + pre[i-1][j]  for j in range(N)])
  
  p, leg_lin, leg_lab = [], [], []
  
  for i in range(n_buckets):
    _p = plt.bar(ind, tuple(pre[i]), width, bottom=tuple(bott[i]))
    leg_lin.append(_p[0])
    leg_lab.append(f"Bucket of {2**i}")
    p.append(_p)

  plt.ylabel('Scores')
  plt.title('Precision by model and bucket')
  plt.xticks(ind, models, rotation=90)
  plt.yticks(np.arange(0, 1.05, 0.05))
  plt.legend(tuple(leg_lin), tuple(leg_lab))
  
  plt.savefig(path + ".png", bbox_inches='tight')
  # plt.savefig(path + ".pdf")

  plt.close('all')

def plot_performance(results, metric, y_label, title):
  """ Plot Performance
  
  Plot a bar graph of the performance of some metric
  
  Arguments:
    metric: the name of the property of the metric
    y_label: the name of the label of the metric
    title: the title of the plot
  """
  
  path = f"{PATH}plots/performance/{title} Performance Bar"
  
  models = tuple(results.keys())
  y_pos = np.arange(len(models))
  performance = [v[metric] for v in results.values()]

  plt.rcdefaults()
  plt.bar(y_pos, performance, align='center', alpha=0.5)
  plt.xticks(y_pos, models, rotation=90)
  plt.ylabel(y_label)
  plt.title(title)

  plt.savefig(path + ".png", bbox_inches='tight')
  # plt.savefig(path + ".pdf")
    
  plt.close('all')

def plot_performance_improved(results, metric, y_label, title):
  """ Plot Performance Improved
  
  Plot a box graph of the performance of some metric
  
  Arguments:
    results: the struct that contain the results of the models
    metric: the name of the property of the metric
    y_label: the name of the label of the metric
    title: the title of the plot
  """
  
  path = f"{PATH}plots/performance/{title} Performance Boxes"
  
  fig, ax_plot = plt.subplots()
  
  ax_plot.set_title(title)
  ax_plot.set_xlabel(y_label)
  ax_plot.set_ylabel('Model')
  
  bplot = ax_plot.boxplot([v['raw'][metric] for v in results.values()], vert=False)
  ax_plot.set_yticklabels(list(results.keys()))
  
  plt.savefig(path + ".png", bbox_inches='tight')
  # plt.savefig(path + ".pdf")
    
  plt.close('all')

def plot_results_comparison(name, xlabel, xticks, metric):
  path = f"{PATH}plots/comparison/{name.lower().replace(' ', '_')}_{metric.lower()}"
  models = [*comparison_data[0]['results']]
  
  for model in models:
    datapoints = [result['results'][model][metric] for result in comparison_data]
    plt.plot(datapoints) 

  plt.title(name)
  plt.ylabel(metric)
  plt.xlabel(xlabel)
  plt.xticks(np.arange(len(xticks)), xticks)
  plt.legend(models, loc='upper left')
  plt.rcdefaults()

  plt.savefig(path + ".png", bbox_inches='tight')
  # plt.savefig(path + ".pdf")
    
  plt.close('all')

def plot_snapshot(results):
  # plot_precision_bucket(results)
  # plot_performance(results, 'TIME', 'Seconds', 'Training Time Comparison')
  plot_performance_improved(results, 'TIME', 'Seconds', 'Training Time Comparison')
  # plot_performance(results, 'RMSE', 'RMSE', 'Root Mean Square Error Comparison')
  plot_performance_improved(results, 'RMSE', 'RMSE', 'Root Mean Square Error Comparison')
  # plot_performance(results, 'NRMSE', 'NRMSE', 'Normalized Root Mean Square Error Comparison')
  # plot_performance_improved(results, 'NRMSE', 'NRMSE', 'Normalized Root Mean Square Error Comparison')
  # plot_performance(results, 'MAE', 'MAE', 'Max Absolute Error Comparison')
  plot_performance_improved(results, 'MAE', 'MAE', 'Max Absolute Error Comparison')
  # plot_performance(results, 'HR', 'Percentage', 'Hit Ratio Comparison')
  plot_performance_improved(results, 'HR', 'Percentage', 'Hit Ratio Comparison')

  for name in results:
    raw = results[name]['raw']

    plot_prediction(raw['expected'], raw['observed'], name)
    
    if 'history' in raw:
      n_hist = len(raw['history'])
      
      for i in range(n_hist):
        plot_history(raw['history'][i], f"{name} ({str(i+1).zfill(2)} of {n_hist})")

# Useful
# https://towardsdatascience.com/using-3d-visualizations-to-tune-hyperparameters-of-ml-models-with-python-ba2885eab2e9
# https://blancas.io/sklearn-evaluation/user_guide/grid_search.html

# Based on:
# https://stackoverflow.com/questions/37161563/how-to-graph-grid-scores-from-gridsearchcv
def plot_grid_search(name, cv_results, grid_param_1, grid_param_2, name_param_1, name_param_2):
    path = f"{PATH}plots/grid/{name}"

    shape = (len(grid_param_2), len(grid_param_1))
    scores_mean = np.array(cv_results['mean_test_score']).reshape(shape)
    scores_sd = np.array(cv_results['std_test_score']).reshape(shape)

    # Plot Grid search scores
    _, ax = plt.subplots(1,1)

    # Param1 is the X-axis, Param 2 is represented as a different curve (color line)
    for idx, val in enumerate(grid_param_2):
        ax.plot(grid_param_1, scores_mean[idx,:], '-o', label= name_param_2 + ': ' + str(val))

    ax.set_title("Grid Search Scores")
    ax.set_xlabel(name_param_1)
    ax.set_ylabel('CV Average Score')
    ax.legend(loc="best", fontsize=15)
    ax.grid('on')

    plt.savefig(path + ".png", bbox_inches='tight')
  # plt.savefig(path + ".pdf")
    
    plt.close('all')

def plot_blocking(n_splits, n_samples):
  k_fold_size = n_samples // n_splits
  
  pre = [[k_fold_size * 0.8] * n_splits, [k_fold_size * 0.2] * n_splits]
  bott = [[], []]

  for i in range(n_splits):
    start = i * k_fold_size
    stop = start + k_fold_size
    mid = int(0.8 * (stop - start)) + start

    bott[0].append(start)
    bott[1].append(mid)

  path = f"{PATH}plots/blocking"

  ind = np.arange(n_splits)    # the x locations for the groups
  p, leg_lin, leg_lab = [], [], []
  
  for i in range(2):
    _p = plt.barh(ind, tuple(pre[i]), left=tuple(bott[i]))
    leg_lin.append(_p[0])
    leg_lab.append("Testing" if i == 1 else "Training")
    p.append(_p)

  plt.title("Blocking Time Series Split ({0} Samples, {1} Splits)".format(n_samples, n_splits))
  plt.xlabel('Samples')
  plt.ylabel('Split')
  plt.xticks(np.arange(0, n_samples+1, n_samples // 5))
  plt.yticks(ind, np.arange(1, n_splits+1))
  plt.legend(tuple(leg_lin), tuple(leg_lab))
  plt.rcdefaults()
    
  plt.savefig(path + ".png")

  plt.close('all')

def print_json (obj):
  print(json.dumps(obj, sort_keys=True, indent=4))

plot_blocking(8, 10000)

name_time = 1573342241

with open(f"{PATH}results/comparison/flow_interval_comparison_{name_time}.json", 'r') as json_file:
  comparison_data = json.load(json_file)

  for result_data in comparison_data:
    plot_snapshot(result_data['results'])

  plot_name = 'Flow Interval for Training Comparison'
  plot_y_label = 'Flow Size in Seconds'
  values = [r['meta']['FLOW_INTERVAL'] for r in comparison_data]

  # plot_results_comparison(plot_name, plot_y_label, values, 'NRMSE')
  plot_results_comparison(plot_name, plot_y_label, values, 'RMSE')
  plot_results_comparison(plot_name, plot_y_label, values, 'MAE')
  plot_results_comparison(plot_name, plot_y_label, values, 'HR')
  plot_results_comparison(plot_name, plot_y_label, values, 'TIME')

name_time = 1573342172

with open(f"{PATH}results/comparison/n_split_comparison_{name_time}.json", 'r') as json_file:
  comparison_data = json.load(json_file)

  for result_data in comparison_data:
    plot_snapshot(result_data['results'])

  plot_name = 'Number of Splits for Training Comparison'
  plot_y_label = 'Number of Splits'
  values = [r['meta']['N_SPLITS'] for r in comparison_data]

  # plot_results_comparison(plot_name, plot_y_label, values, 'NRMSE')
  plot_results_comparison(plot_name, plot_y_label, values, 'RMSE')
  plot_results_comparison(plot_name, plot_y_label, values, 'MAE')
  plot_results_comparison(plot_name, plot_y_label, values, 'HR')
  plot_results_comparison(plot_name, plot_y_label, values, 'TIME')

name_time = 1573342238

with open(f"{PATH}results/comparison/predict_future_comparison_{name_time}.json", 'r') as json_file:
  comparison_data = json.load(json_file)

  for result_data in comparison_data:
    plot_snapshot(result_data['results'])

  plot_name = 'Predict Future for Training Comparison'
  plot_y_label = 'Time in the Future in Minutes'
  values = [r['meta']['PREDICT_IN_FUTURE'] for r in comparison_data]

  # plot_results_comparison(plot_name, plot_y_label, values, 'NRMSE')
  plot_results_comparison(plot_name, plot_y_label, values, 'RMSE')
  plot_results_comparison(plot_name, plot_y_label, values, 'MAE')
  plot_results_comparison(plot_name, plot_y_label, values, 'HR')
  plot_results_comparison(plot_name, plot_y_label, values, 'TIME')

name_time = 1573342180

with open(f"{PATH}results/comparison/seeable_past_comparison_{name_time}.json", 'r') as json_file:
  comparison_data = json.load(json_file)

  for result_data in comparison_data:
    plot_snapshot(result_data['results'])

  plot_name = 'Seeable Past for Training Comparison'
  plot_y_label = 'Seeable Past in Seconds'
  values = [r['meta']['SEEABLE_PAST'] for r in comparison_data]

  # plot_results_comparison(plot_name, plot_y_label, values, 'NRMSE')
  plot_results_comparison(plot_name, plot_y_label, values, 'RMSE')
  plot_results_comparison(plot_name, plot_y_label, values, 'MAE')
  plot_results_comparison(plot_name, plot_y_label, values, 'HR')
  plot_results_comparison(plot_name, plot_y_label, values, 'TIME')

names = {
    # 'RF Grid A',
    # 'RF Grid B',
    'SVM Grid A',
    # 'SVM Grid B',
    # 'LSTM Grid A',
    # 'LSTM Grid B',
    # 'GRU Grid A',
    # 'GRU Grid B',
}

for name in names:
  with open(f"{PATH}results/grid/{name}.json", 'r') as json_file:
    grid_result = json.load(json_file)

    keys = list(grid_result['params'].keys())
    values = list(grid_result['params'].values())

    plot_grid_search(name, grid_result['score'], values[0], values[1], keys[0], keys[1])