# -*- coding: utf-8 -*-
"""tcc_data_processing.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1AMYaNffC8F3Q5ENvme4kHneHL-Ysjb4N
"""

import google as g # To connect with google drive
g.colab.drive.mount('/content/drive')

!pip install numpy
!pip install pandas
!pip install statsmodels

import pandas as pd # data manipulation library
import numpy as np # math library
import datetime as dt # to discover week day
import statsmodels as sm # statistical models
import statsmodels.api as sma # statistical models api

TCC_FOLDER = '/content/drive/My Drive/TCC/'

DATASET_PATH = f"{TCC_FOLDER}dataset/all_data_sorted.csv"

col_names = [
  'Sensor',
  'Date',
  'Time',
  'Lane',
  'Speed',
  'Max Speed',
  'Size'
]

data = pd.read_csv(DATASET_PATH, ';', header=None, names=col_names)

data.head()

data.describe()

print(f"It contains {len(data['Sensor'])} entries\n\n")

print(f"This dataset contains {len(set(data['Sensor']))} sensors.")

for val in set(data['Sensor']):
  print(f"Sensor {val} has {len(data[data['Sensor'] == val])}")

# Limit sensor usage
data = data[data['Sensor'] == 'RSI128']

# Remove unnecessary columns
data = data.drop(columns=['Sensor','Lane','Max Speed','Size'])

# Get datetime
data['Date'] = pd.to_datetime(data['Date'], format='%Y/%m/%d')

# Adjust type
f = lambda x : tm.strptime(x, '%H:%M:%S')
data['Time'] = data['Time'].apply(f)

g = lambda x : dt.timedelta(hours=x.tm_hour,minutes=x.tm_min,seconds=x.tm_sec).total_seconds()
data['Time'] = data['Time'].apply(g)

h = lambda x : int(x)
data['Time'] = data['Time'].apply(h)

# Create week day from date
j = lambda x : x.weekday()
data['WeekDay'] = data['Date'].apply(j)

data.head()

data.describe()

for col, cont in data.iteritems():
  print(f"Column {col} has {cont.isnull().sum()} null elements")
  print(f"Column {col} has {cont.isna().sum()} nan elements")

start = data['Date'].min()
end = data['Date'].max()

print(f"\nThis data is from <{start}> to <{end}>. {(end - start).days + 1} days.\n")
print(f"It contains {len(data['Date'])} entries\n\n")

data.to_csv(f"{TCC_FOLDER}dataset/dataset.csv", ";", index=False)

def get_day_size (flow_interval):
  return (24 * 60 * 60) // flow_interval

def get_week_size (flow_interval):
  return (7 * 24 * 60 * 60) // flow_interval

def get_flow_data(n, accSpeed, weekDay):
  avgSpeed = (accSpeed / n) if n else 0
  density = (n / avgSpeed) if avgSpeed else 0
  w = [(1 if weekDay == i else 0) for i in range(7)] # weekday
  
  return (n, density, avgSpeed, w[0], w[1], w[2], w[3], w[4], w[5], w[6])

def get_flow (data, flow_interval):
  date = np.asarray(data['Date'])
  weekDay = np.asarray(data['WeekDay'])
  time = np.asarray(data['Time'])
  speed = np.asarray(data['Speed'])
  
  dateControl = date[0]
  timeBlock = flow_interval
  countFlow = 0
  accSpeed = 0
  flowData = []

  for i in range(len(date)):
    if time[i] >= timeBlock: # init a new time block
      flowData.append(get_flow_data(countFlow, accSpeed, weekDay[i])) 
      timeBlock += flow_interval
      accSpeed = 0
      countFlow = 0
      
    if date[i] > dateControl: # reset on day change
      dateControl = date[i]
      timeBlock = flow_interval 
      countFlow = 0
      accSpeed = 0
      
    if time[i] < timeBlock: # add car on flow
      countFlow += 1
      accSpeed += speed[i]

  day_size = get_day_size(flow_interval)
  k = (day_size - (len(flowData) % day_size)) % day_size

  for i in range(k):
    flowData.append(get_flow_data(0, 0, weekDay[len(date) - 1])) 
      
  cols = [
    'Flow',
    'Density',
    'AveSpeed',
    'Sunday',
    'Monday',
    'Tuesday',
    'Wednesday',
    'Thursday',
    'Friday',
    'Saturday',
  ]
  
  flowData = pd.DataFrame(flowData, columns=cols)
  
  # from sklearn.preprocessing import MinMaxScaler
  # scaler = MinMaxScaler(feature_range=(0,1))
  # flowDataScaled = scaler.fit_transform(flowData)  
  # flowData = pd.DataFrame(flowDataScaled, columns=flowData.columns, index=flowData.index)
  
  return flowData

def plot_flow_decomposition(flow_series, freq, flow_interval):
  path = f"{TCC_FOLDER}plots/flow/seasonal_decompose_{flow_interval}"
  
  decompose = sm.tsa.seasonal.seasonal_decompose
  decomposition = decompose(flow_series, model='additive', freq=freq)
  fig = decomposition.plot()

  plt.rcdefaults()
  
  plt.savefig(path + ".png")
  plt.savefig(path + ".pdf")
    
  plt.close('all')

def plot_flow(flow_series, flow_interval):
  """ Plot of Flow
  
  Plot the flow from week to week
  
  Arguments:
    flow_series: an array of flows
    flow_interval: the interval in which the flow was made
  """

  week_size = get_week_size(flow_interval)
  n = len(flow_series) // week_size

  if len(flow_series) % week_size == 0:
    print('Yey')

  for i in range(n):
    s = week_size * i
    e = min(s + week_size, len(flow_series))
    path = f"{TCC_FOLDER}plots/flow/flow_{flow_interval}_week_{str(i+1).zfill(2)}"

    plt.plot(flow_series[s:e])

    plt.title(f"Flow ({flow_interval}s) - Week {i+1}")
    plt.ylabel('Flow')
    plt.xlabel('Time')
    plt.rcdefaults()
    
    plt.savefig(path + ".png", bbox_inches='tight')
    plt.savefig(path + ".pdf")
    
    plt.close('all')

flows_intervals = [150, 300, 450, 900]

for flow_interval in flows_intervals:
  flow_data = get_flow(data, flow_interval)
  week_size = get_week_size(flow_interval)

  plot_flow(flow_data['Flow'], flow_interval)
  plot_flow_decomposition(flow_data['Flow'], week_size, flow_interval)
  print(flow_data.head(), end="\n\n")
  print(flow_data.describe(), end="\n\n")

  flow_data.to_csv(f"{TCC_FOLDER}dataset/dataset_flow_{flow_interval}.csv", ";", index=False)