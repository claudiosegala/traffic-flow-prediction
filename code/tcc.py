# -*- coding: utf-8 -*-
"""tcc.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1fpyYtJyD0OtH2pP_DKB9QpaTzfUICiMu

# TCC

## Models

- Random Guess
- Mean
- Random Forest
- RNN
- LSTM
- GRU

## Evaluation

- RMSE
- NRMSE
  - Normalized by stardartization

# Mount Drive

Connect to Google Drive of 'alfredcoinworth'
"""

import google as g # To connect with google drive
g.colab.drive.mount('/content/drive')

"""# Install Dependencies

In this phase we have to download all the dependencies that our code will need
"""

!pip install tensorflow
!pip install pandas
!pip install matplotlib
!pip install numpy
!pip install sklearn
!pip install keras
!pip install statsmodels

"""# Define headers

In this phase we have to declare all the libraries that we will use.
"""

import tensorflow as tf # machine learning library
import pandas as pd # data manipulation library
import matplotlib.pyplot as plt # plot library
import numpy as np # math library
import datetime as dt # to discover week day
import time as tm # to convert to seconds
import sklearn as skl # regression templates library
import sklearn.metrics as sklm # metrics
import statsmodels.api as sma # statistical models api
import statistics as st # statistics
import statsmodels as sm # statistical models

"""# Configurations

Diminish the verbosity of tensorflow
"""

tf.logging.set_verbosity(tf.logging.ERROR)

"""# Util"""

def plot_prediction (Y, Y_hat):
  plt.plot(Y)
  plt.plot(Y_hat)
  plt.title('Prediction')
  plt.ylabel('Flow')
  plt.xlabel('Time')
  plt.legend(['actual', 'prediction'], loc='upper left')
  plt.rcdefaults()
  plt.show('Prediction')
  plt.close('all')

def plot_history (history, name):
  plt.plot(history.history['loss'])
  plt.plot(history.history['val_loss'])
  plt.title(name + ' Model Loss')
  plt.ylabel('Loss')
  plt.xlabel('Epoch')
  plt.legend(['train', 'test'], loc='upper left')
  plt.figure(figsize=(100, 100))
  plt.show(name + "ind")
  plt.close('all')

def print_metrics (Y_hat, Y_test):
  #evs = sklm.explained_variance_score(Y_test, Y_hat)
  mse = sklm.mean_squared_error(Y_test, Y_hat)
  mae = sklm.mean_absolute_error(Y_test, Y_hat)
  #msle = sklm.mean_squared_log_error(Y_test, Y_hat)
  me = sklm.max_error(Y_test, Y_hat)
  
  #print(f"EVS: {evs}")
  print(f"RMSE: {np.sqrt(mse)}")
  #print(f"MSE: {mse}")
  #print(f"MAE: {mae}")
  #print(f"MSLE: {msle}")
  #print(f"Max Error: {me}")

"""# Configure Hyperparameters"""

TRAIN_SPLIT = 0.8

SEEABLE_PAST = 100 # in minutes

PREDICT_IN_FUTURE = 15 # in minutes

FLOW_INTERVAL = 150 # the interval size for each flow

MULTIVARIATE = False # if we are gonna use multiple data or not

# Derivated

N_STEPS = SEEABLE_PAST * 60 // FLOW_INTERVAL # the number of flows to see in the past

N_FUTURE = PREDICT_IN_FUTURE * 60 // FLOW_INTERVAL # how much in the future we want to predict (0 = predict the flow on the next 5 minutes)

N_FEATURES = 9 if MULTIVARIATE else 1

DAY_SIZE = (24 * 3600) // FLOW_INTERVAL  

WEEK_SIZE = 7 * DAY_SIZE

"""# Data Retrieval

This phase we have to get the data stored in Google Drive.
"""

col_names = ['Sensor', 'Date', 'Time', 'Lane', 'Speed', 'Max Speed', 'Size']
all_data = pd.read_csv('/content/drive/My Drive/TCC/chunks/all_data_sorted.csv', ';', header=None, names=col_names)

all_data.head()

"""# Data Pre-processing"""

def adjust_data(data):
  # Extract data from just one sensor
  data = data[data['Sensor'] == 'RSI128']
  
  # Remove unnecessary columns
  data = data.drop(columns=['Sensor','Lane','Max Speed','Size'])
   
  # Adjust type
  data['Time'] = data['Time'].apply(lambda x : tm.strptime(x, '%H:%M:%S'))
  data['Time'] = data['Time'].apply(lambda x : dt.timedelta(hours=x.tm_hour,minutes=x.tm_min,seconds=x.tm_sec).total_seconds())
  data['Time'] = data['Time'].apply(lambda x : int(x))
  data['Date'] = pd.to_datetime(data['Date'], format='%Y/%m/%d')
  data['Speed'].apply(lambda x : float(x))
  
  # Create week day from date
  data['WeekDay'] = data['Date'].apply(lambda x : x.weekday())
  
  return data

data = adjust_data(all_data)

data.head()

data.describe()

print(f"This data is from <{data['Date'].min()}> to <{data['Date'].max()}>\n")

print(f"It contains {len(data['Date'])} entries\n")

for col, cont in data.iteritems():
    print(f"Column {col} has {cont.isnull().sum()} null elements and {cont.isna().sum()} nan elements")

"""# Flow Retrieval

This will transform the time series of register cars that passed in a array of flow per k minute.
"""

def get_flow_data(n, accSpeed, weekDay):
  avgSpeed = accSpeed // n if n else 0
  wd = [1 if weekDay == i else 0 for i in range(7)] # weekday
  
  return (n, avgSpeed, wd[0], wd[1], wd[2], wd[3], wd[4], wd[5], wd[6])

def get_flow (data):
  date = np.asarray(data['Date'])
  weekDay = np.asarray(data['WeekDay'])
  time = np.asarray(data['Time'])
  speed = np.asarray(data['Speed'])
  
  dateControl = date[0] #seta o controle de data com o primeiro dia do chunk
  timeBlock = FLOW_INTERVAL
  countFlow = 0
  accSpeed = 0
  flowData = []

  for i in range(len(date)):
    if time[i] >= timeBlock: # init a new time block
      flowData.append(get_flow_data(countFlow, accSpeed, weekDay[i])) 
      timeBlock += FLOW_INTERVAL
      accSpeed = 0
      countFlow = 0
      
    if date[i] > dateControl: # reset on day change
      dateControl = date[i]
      timeBlock = FLOW_INTERVAL 
      countFlow = 0
      accSpeed = 0
      
    if time[i] < timeBlock: # add car on flow
      countFlow += 1
      accSpeed += speed[i]
      
  flowDataColumns = ['Flow', 'AverageSpeed', 'Sunday', 'Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday']
  flowData = pd.DataFrame(flowData, columns=flowDataColumns) 
  
  return flowData

multivariateData = get_flow(data)
univariateData = multivariateData['Flow']

multivariateData.describe()

univariateData.describe()

"""# Flow Analysis

This is based on [A Guide to Time Series Visualization with Python 3](https://www.digitalocean.com/community/tutorials/a-guide-to-time-series-visualization-with-python-3).
"""

decomposition = sm.tsa.seasonal.seasonal_decompose(univariateData, model='additive', freq=WEEK_SIZE)
fig = decomposition.plot()

plt.rcdefaults()
plt.plot()

"""# Prepare Dataset

This phase will form the input (X) and the output (Y)
"""

# split_sequence([1, 2, 3, 4, 5], 3) #=> ([[1, 2, 3], [2, 3, 4]], [4, 5])
def split_sequence(sequence):
  n = len(sequence)
  X, Y = list(), list()
  
  for i in range(n):
    # find the end of this pattern
    end_ix = i + N_STEPS

    # check if we are beyond the sequence
    if end_ix + N_FUTURE > n-1:
      break

    # gather input and output parts of the pattern
    seq_x, seq_y = sequence[i:end_ix], sequence[end_ix + N_FUTURE]
    X.append(seq_x)
    Y.append(seq_y[0] if MULTIVARIATE else seq_y)

  return np.array(X), np.array(Y)

def reshape_flow (raw_seq):  
  # split into samples
  X, Y = split_sequence(raw_seq)

  # reshape from [samples, timesteps] into [samples, timesteps, features]
  X = X.reshape((X.shape[0], X.shape[1], N_FEATURES))
  
  return X, Y

X, Y = reshape_flow(multivariateData if MULTIVARIATE else univariateData)

print(X.shape)

print(Y.shape)

"""# Preparation for Statistical Methods"""

def evaluate_method (Y, Y_hat, start):
  time = tm.time() - start
  rmse = np.sqrt(sklm.mean_squared_error(Y, Y_hat))
  nrmse = rmse / np.std(Y)
  mae = sklm.mean_absolute_error(Y, Y_hat)
  
  print(f"Training Time: {int(time)}s, RMSE: {rmse}, NRMSE: {nrmse}, MAE: {mae}")
  
  return (time, rmse, nrmse, mae)

"""# Baseline: Random

This implementation just guess a random number in the [0, 100] interval for every output.
"""

import random as rnd # random

def random_guess (Y):
  start = tm.time()
  m = max(Y)
  
  Y_hat = [rnd.randint(0, m) for i in range(len(Y))]
  
  plot_prediction(Y, Y_hat)
  
  return evaluate_method(Y, Y_hat, start)

"""# Baseline: Default

This implementation just get the mean of every flow value in the input and place it as output.
"""

def default_mean (X, Y):
  start = tm.time()
  
  Y_hat = [np.mean([v[0] for v in x]) for x in X]
  
  plot_prediction(Y, Y_hat)
  
  return evaluate_method(Y, Y_hat, start)

"""# Random Forest

This implementation is based on [Random Forest Algorithm with Python and Scikit-Learn](https://stackabuse.com/random-forest-algorithm-with-python-and-scikit-learn/)
"""

from sklearn.ensemble.forest import RandomForestRegressor

def random_forest(X, Y):
  X = X.reshape(X.shape[0], X.shape[1])
  
  train_sz = int(len(Y) * 0.8)
  
  X_train, X_test, Y_train, Y_test = skl.model_selection.train_test_split(X, Y, test_size=0.2, random_state=0)
  
  model = skl.ensemble.RandomForestRegressor(n_estimators=100, max_features='auto', random_state=0)
  
  start = tm.time()
  
  model.fit(X_train, Y_train)
  
  Y_hat = model.predict(X_test)
  
  plot_prediction(Y_test, Y_hat)
  
  return evaluate_method(Y_test, Y_hat, start)

"""# Support Vector Machine"""

from sklearn import svm

def support_vector_machine(X,Y):
  X_train, X_test, Y_train, Y_test = skl.model_selection.train_test_split(X, Y, test_size=0.2, random_state=0)
  
  X_train = X_train.reshape((X_train.shape[0], X_train.shape[1]))

  X_test = X_test.reshape((X_test.shape[0], X_test.shape[1]))

  clf = svm.SVR(gamma='scale', C=1.0, epsilon=0.2)
  
  start = tm.time()
  
  model.fit(X_train, Y_train)
  
  Y_hat = model.predict(X_test)
  
  plot_prediction(Y_test, Y_hat)
  
  return evaluate_method(Y_test, Y_hat, start)

"""# Preparation for Deep Learning"""

from keras.models import Sequential
from keras.layers import SimpleRNN, Dense

def evaluate_model (model, X, Y, train_size=20000, test_size=2000, validation_split=0.25, batch_size=64, epochs=15, verbose=0):
  RMSE, NRMSE, MAE = [], [], []
  size = len(X)
  i, j, k = 0, train_size, min(size, train_size + test_size)
  start = tm.time()
  jump_size = int(train_size * validation_split)
  
  while j < size:
    _start = tm.time()
    
    hist = model.fit(X[i:j], Y[i:j], validation_split=validation_split, batch_size=batch_size, epochs=epochs, verbose=verbose)
    
    Y_hat = model.predict(X[j:k], verbose=verbose)
    
    rmse = np.sqrt(sklm.mean_squared_error(Y[j:k], Y_hat))
    
    nrmse = rmse / np.std(Y[j:k])
    
    mae = sklm.mean_absolute_error(Y[j:k], Y_hat)
  
    RMSE.append(rmse)
    
    NRMSE.append(nrmse)
    
    MAE.append(mae)
    
    if verbose > 1:
      print(f" - {len(RMSE)}/{(size - train_size + jump_size - 1) // jump_size} ({tm.time() - _start}s) - RMSE: {rmse}, NRMSE: {nrmse}")
      
    if i == 0:
      plot_prediction(Y[j:k], Y_hat)
      
    i = i + jump_size
    j = i + train_size
    k = min(size, j + test_size)
  
  time = tm.time() - start
  rmse = sum(RMSE) / len(RMSE)
  nrmse = sum(NRMSE) / len(NRMSE)
  mae = sum(MAE) / len(MAE)
  
  print(f"Training Time {int(time)}s, RMSE: {rmse}, NRMSE: {nrmse}, MAE: {mae}")
  
  return (time, rmse, nrmse, mae)

results = {}

"""# RNN"""

from keras.layers import SimpleRNN

def rnn (X, Y): 
  model = Sequential()
  
  model.add(SimpleRNN(50, activation='relu', input_shape=(N_STEPS, N_FEATURES)))
  model.add(Dense(1))
  
  model.compile(optimizer='adam', loss='mse', metrics = ["accuracy"])
  
  return evaluate_model(model, X, Y)

"""# LSTM"""

from keras.layers import LSTM

def lstm (X, Y): 
  model = Sequential()
  
  model.add(LSTM(50, activation='relu', input_shape=(N_STEPS, N_FEATURES)))
  model.add(Dense(1))
  
  model.compile(optimizer='adam', loss='mse', metrics = ["accuracy"])

  return evaluate_model(model, X, Y)

"""# GRU"""

from keras.layers import GRU

def gru (X, Y): 
  model = Sequential()
  
  model.add(GRU(50, activation='relu', input_shape=(N_STEPS, N_FEATURES)))
  model.add(Dense(1))
  
  model.compile(optimizer='adam', loss='mse', metrics = ["accuracy"])

  return evaluate_model(model, X, Y)

"""# Running"""

results['Random'] = random_guess(Y)

results['Mean'] = default_mean(X, Y)

results['RF'] = random_forest(X, Y)

results['SVM'] = support_vector_machine(X, Y)

results['RNN'] = rnn(X, Y)

results['LSTM'] = lstm(X, Y)

results['GRU'] = gru(X, Y)

"""# Results"""

def plot_performance(i, y_label, title):
  models = tuple(results.keys())
  y_pos = np.arange(len(models))
  print(results.values())
  performance = [v[i] for v in results.values()]

  plt.rcdefaults()
  plt.bar(y_pos, performance, align='center', alpha=0.5)
  plt.xticks(y_pos, models)
  plt.ylabel(y_label)
  plt.title(title)

  plt.show()

plot_performance(0, 'Seconds', 'Training Time Comparison')

plot_performance(1, 'RMSE', 'Root Mean Square Error Comparison')

plot_performance(2, 'NRMSE', 'Normalized Root Mean Square Error Comparison')

plot_performance(3, 'MAE', 'Max Absolute Error Comparison')

"""# Observations:

+ For the evaluation of the RNN and it's variations was used the Walking Forward methodology so that we had many test sessions and all training sessions where the same size [[1]](https://towardsdatascience.com/time-series-nested-cross-validation-76adba623eb9)
"""