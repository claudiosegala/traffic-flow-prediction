Após o tratamento dos dados e da definição dos melhores parâmetros e hiperparâmetros para cada modelo, foram realizados diversos testes e comparações. Todos os experimentos foram realizados em uma máquina equipada com processador intel I7 7700HQ, 8 Gb ram rodando Ubuntu 18.05.

A primeira comparação feita foi entre os resultados das predições dos modelos com o valores de entrada normalizados e não normalizados. Para os modelos univariados, normalizamos o valor do fluxo utilizando a biblioteca sklearn, onde o menor valor é mapeado como 0 e o maior valor como 1. Segundo [Citar fonte que mostra que dados normalizados tendem a ter um melhor resultado], valores normalizados entre 0 e 1 tendem a ter um melhor resultado para algoritmos que utilizam funções de ativação sigmoidais, além de evitar que o modelo fique enviesado para os valores com maiores ordens de grandeza. Porém, como pode ser visto na tabela 1, no caso do \archfull{LSTM}, o \archfull{RMSE} ficou levemente pior para as predições utilizando valores normalizados em seu treinamento. Isso pode ter acontecido pois o valor do intervalo do fluxo oriundo dos nossos dados não era tão grande, sendo o menor valor 0 e o maior valor 60. Com um intervalo pequeno como esse, a normalização não teve um impacto tão grande, pois 0 e 60 estão apenas a uma ordem de grandeza de diferença.

O mesmo raciocínio pode ser utilizado para explicar a tabela 2, onde podemos observar que a normalização também não trouxe grandes benefícios aos modelos multivariados. Novamente, a ordem de grandeza entre os dados de entrada e entre os seus diferentes tipos (velocidade, fluxo) é bastante pequena, tornando a normalização pouco eficaz também nos casos dos modelos multivariados.

Como a normalização não trouxe muitas melhorias aos modelos, decidimos por continuar os testes utilizando os valores não normalizados. O próximo passo foi decidir os melhores valores para cada parâmetro e hiperparâmetro. Como pode ser observado nos gráficos 1  a 7, todos os modelos tiveram uma melhora depois de passarem por um teste utilizando o Hyperas.

Com todos os modelos nas suas melhores versões, com os valores mais adequados de parâmetros e hiperparâmetros, rodamos todos os modelos com o mesmo dataset e para os seguintes valores de Janela: 


Como pode ser visto na tabela X,  todos  os  modelos  de aprendizagem  profunda  (LSTM  uni-variado  e  multi-variado,RNN  e  GRU)  tiveram  resultados  semelhantes,  mas  não  tão bons quanto os de aprendizagem supervisionada comum, como o random forest. Isso  pode  ter  acontecido  devido  ao  nosso  conjunto  de dados  e  seu  tamanho.  Redes  neurais  recorrentes  precisam de  um  grande  volume  de  dados  para  mapear  e  aprender  a sua  distribuição,  ao  contrário  de  modelos  de  aprendizagem supervisionada  tradicionais, que obtiveram melhores previsões com o nosso dataset.

Outro  ponto  interessante  a  se  notar ao observar a tabela y  é  o  tempo  de  treinamento de cada método. Os modelos de aprendizagem profunda tiveram um tempo de treinamento consideravelmente maior se comparado aos demais, o que é esperado, visto que possuem muito mais camadas de processamento. Já os  modelos  utilizados  como  base  de  comparação  tiveram um  tempo  de treinamento  extremamente  rápido,  pois  são métodos triviais  e  não  exigem  muito  processamento  e,  por consequência, também tiveram as piores previsões.
